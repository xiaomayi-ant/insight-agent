from __future__ import annotations

import json
from typing import Any, Dict, Optional

from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.runnables import RunnableConfig
from langchain_community.chat_models import ChatTongyi
from pydantic import BaseModel

from src.core.settings import AppSettings
from src.domain.state import FrontendSearchInput
from src.graphs.vkdb_graph.tools import make_vkdb_search_tool
from src.services.vkdb_mysql_service import vkdb_response_to_mysql_join


def intent_analysis_node(settings: AppSettings):
    """æ„å›¾åˆ†æèŠ‚ç‚¹ï¼šåˆ¤æ–­æ˜¯ç®€å•èŠå¤©è¿˜æ˜¯éœ€è¦VikingDBæœç´¢"""
    
    llm = ChatTongyi(model=settings.qwen_model, temperature=settings.qwen_temperature)
    
    def _node(state: Dict[str, Any]) -> Dict[str, Any]:
        import logging
        logger = logging.getLogger(__name__)
        logger.info("ğŸ” [èŠ‚ç‚¹æ‰§è¡Œ] intent_analysis - å¼€å§‹æ„å›¾åˆ†æ")
        
        # ç›´æ¥è®¿é—®å­—å…¸ï¼ŒMessagesStateæ˜¯TypedDict
        messages = state.get("messages", [])
        last_message = messages[-1] if messages else None
        
        if not last_message:
            logger.error("âŒ [èŠ‚ç‚¹é”™è¯¯] intent_analysis - æ²¡æœ‰æ¶ˆæ¯")
            return {"error": "No message provided"}
        
        user_input = last_message.content
        logger.info(f"ğŸ“ [èŠ‚ç‚¹è¾“å…¥] intent_analysis - ç”¨æˆ·è¾“å…¥: {user_input[:50]}...")
        
        # ä½¿ç”¨LLMåˆ¤æ–­æ„å›¾ï¼Œå¹¶å°½é‡æå– influencer åç§°
        system_prompt = """ä½ æ˜¯æ„å›¾åˆ†æåŠ©æ‰‹ï¼Œå¿…é¡»è¿”å› JSONã€‚

è§„åˆ™ï¼š
- å¦‚æœç”¨æˆ·åœ¨æ‰¾è§†é¢‘/æ•°æ®/å½±å“è€…/æœç´¢/æŸ¥è¯¢/æŸ¥æ‰¾/åˆ†æï¼Œæ„å›¾ä¸º vkdb_searchã€‚
- å¦‚æœç”¨æˆ·åªæ˜¯é—²èŠï¼Œæ„å›¾ä¸º simple_chatã€‚

è¾“å‡º JSON ç»“æ„ï¼ˆå­—æ®µåå›ºå®šï¼Œå…¨éƒ¨å°å†™ï¼‰ï¼š
{
  "intent": "vkdb_search" | "simple_chat",
  "query": "å°½é‡æå–çš„æ ¸å¿ƒæŸ¥è¯¢è¯ï¼Œå¦‚ï¼šæè¯çš„è§†é¢‘",    // vkdb_search å¿…å¡«
  "influencer": "å½±å“è€…å§“åï¼Œå¦‚æœèƒ½ç¡®å®šå°±å¡«ï¼Œæ¯”å¦‚ï¼šæè¯ï¼›å¦åˆ™ç•™ç©º" // å¯é€‰
}

æ³¨æ„ï¼š
- åªè¾“å‡º JSONï¼Œä¸è¦å…¶å®ƒå†…å®¹ã€‚
- å¦‚æœèƒ½ç¡®å®š influencer å°±å¡«å…·ä½“å§“åï¼Œå¦åˆ™å¡«ç©ºå­—ç¬¦ä¸²ã€‚
"""
        
        analysis_prompt = f"ç”¨æˆ·è¾“å…¥ï¼š{user_input}\n\nè¯·æŒ‰ç…§ä¸Šè¿° JSON ç»“æ„è¿”å›ã€‚"
        
        try:
            # ä½¿ç”¨ç»“æ„åŒ–è¾“å‡º
            class IntentResult(BaseModel):
                intent: str
                query: Optional[str] = None
                influencer: Optional[str] = None
            
            structured_llm = llm.with_structured_output(IntentResult)
            result = structured_llm.invoke([
                SystemMessage(content=system_prompt),
                HumanMessage(content=analysis_prompt)
            ])
            
            if result.intent == "vkdb_search":
                # ä¼˜å…ˆä½¿ç”¨è§£æå‡ºçš„ influencerï¼Œå¦åˆ™ç”¨ queryï¼Œå¦åˆ™å›é€€ user_input
                influencer = (result.influencer or "").strip()
                query = (result.query or influencer or user_input).strip()
                logger.info(f"âœ… [èŠ‚ç‚¹ç»“æœ] intent_analysis - æ„å›¾: vkdb_search, æŸ¥è¯¢: {query}, influencer: {influencer}")
                return {
                    "intent": "vkdb_search",
                    "vkdb_query": query,
                    "vkdb_influencer": influencer or None
                }
            else:
                logger.info(f"âœ… [èŠ‚ç‚¹ç»“æœ] intent_analysis - æ„å›¾: simple_chat")
                return {
                    "intent": "simple_chat"
                }
        except Exception as e:
            logger.warning(f"âš ï¸ [èŠ‚ç‚¹è­¦å‘Š] intent_analysis - ç»“æ„åŒ–è¾“å‡ºå¤±è´¥ï¼Œä½¿ç”¨fallback: {e}")
            # å¦‚æœç»“æ„åŒ–è¾“å‡ºå¤±è´¥ï¼Œä½¿ç”¨ç®€å•è§„åˆ™ä½œä¸ºfallback
            vkdb_keywords = ["æœç´¢", "æŸ¥æ‰¾", "æŸ¥è¯¢", "æ‰¾", "æŸ¥", "æ•°æ®", "è§†é¢‘", "å½±å“è€…", "åˆ†æ"]
            if any(keyword in user_input for keyword in vkdb_keywords):
                logger.info(f"âœ… [èŠ‚ç‚¹ç»“æœ] intent_analysis - æ„å›¾: vkdb_search (fallback)")
                return {
                    "intent": "vkdb_search",
                    "vkdb_query": user_input
                }
            logger.info(f"âœ… [èŠ‚ç‚¹ç»“æœ] intent_analysis - æ„å›¾: simple_chat (fallback)")
            return {
                "intent": "simple_chat"
            }
    
    return _node


def vkdb_search_node(settings: AppSettings):
    """VikingDBæœç´¢èŠ‚ç‚¹"""
    
    tool = make_vkdb_search_tool(settings)
    
    def _node(state: Dict[str, Any]) -> Dict[str, Any]:
        import logging
        logger = logging.getLogger(__name__)
        logger.info("ğŸ” [èŠ‚ç‚¹æ‰§è¡Œ] vkdb_search - å¼€å§‹VikingDBæœç´¢")
        
        # ç›´æ¥è®¿é—®å­—å…¸
        intent = state.get("intent")
        if intent != "vkdb_search":
            logger.error(f"âŒ [èŠ‚ç‚¹é”™è¯¯] vkdb_search - æ— æ•ˆæ„å›¾: {intent}")
            return {"error": "Invalid intent for vkdb_search node"}
        
        query = state.get("vkdb_query") or ""
        influencer_hint = (state.get("vkdb_influencer") or "").strip()
        if not query:
            logger.error("âŒ [èŠ‚ç‚¹é”™è¯¯] vkdb_search - æ²¡æœ‰æŸ¥è¯¢å†…å®¹")
            return {"error": "No query provided"}
        
        logger.info(f"ğŸ“ [èŠ‚ç‚¹è¾“å…¥] vkdb_search - æŸ¥è¯¢: {query}")
        if influencer_hint:
            logger.info(f"ğŸ“Œ [èŠ‚ç‚¹è¾“å…¥] vkdb_search - è§£æå‡ºçš„influencer: {influencer_hint}")
        
        # ä»settingsè¯»å–é…ç½®
        vkdb_limit = settings.vikingdb_default_limit
        # è§£æè¾“å‡ºå­—æ®µï¼ˆä»ç¯å¢ƒå˜é‡æˆ–é»˜è®¤å€¼ï¼‰
        # Agent Graph éœ€è¦çš„å­—æ®µï¼šinfluencer, intent_analysis, landscape_video
        # ä¼˜å…ˆä½¿ç”¨ VKDB_AGENT_OUTPUT_FIELDSï¼Œå¦‚æœæ²¡æœ‰è®¾ç½®åˆ™ä½¿ç”¨é»˜è®¤å¿…éœ€å­—æ®µ
        if settings.vkdb_output_fields:
            from src.infra.vkdb.client import parse_output_fields
            output_fields = parse_output_fields(settings.vkdb_output_fields)
        else:
            # é»˜è®¤ä½¿ç”¨Agent Graphå¿…éœ€çš„å­—æ®µï¼ˆè¿™äº›å­—æ®µæ˜¯Agent Graphå·¥ä½œæµå¿…éœ€çš„ï¼‰
            # æ³¨æ„ï¼šå¦‚æœéœ€è¦ä¿®æ”¹è¿™äº›é»˜è®¤å­—æ®µï¼Œè¯·åœ¨ .env ä¸­è®¾ç½® VKDB_AGENT_OUTPUT_FIELDS
            output_fields = ["influencer", "intent_analysis", "landscape_video"]
        
        # ç®€åŒ–å‚æ•°ï¼šåªä½¿ç”¨influencer
        # æ³¨æ„ï¼šVikingDBè¦æ±‚text/image/videoè‡³å°‘æœ‰ä¸€ä¸ªï¼Œæ‰€ä»¥è®¾ç½®text=influence
        influence_value = influencer_hint or query  # ä¼˜å…ˆä½¿ç”¨æ„å›¾è§£æå‡ºçš„ influencer
        search_input = FrontendSearchInput(
            influence=influence_value,  # influenceråç§°
            text=query,  # åŒæ—¶è®¾ç½®textï¼ˆVikingDB APIè¦æ±‚ï¼‰
            limit=vkdb_limit,
            output_fields=output_fields
        )
        
        logger.info(f"ğŸ“‹ [èŠ‚ç‚¹å‚æ•°] vkdb_search - influencer: {influence_value}, text: {query}, limit: {vkdb_limit}, output_fields: {search_input.output_fields}")
        
        # è°ƒè¯•ï¼šæŸ¥çœ‹å®é™…ä¼ é€’ç»™å·¥å…·çš„å‚æ•°
        tool_params = search_input.model_dump(exclude_none=True)
        logger.info(f"ğŸ” [èŠ‚ç‚¹è°ƒè¯•] vkdb_search - å·¥å…·å‚æ•°: {tool_params}")
        logger.info(f"ğŸ” [èŠ‚ç‚¹è°ƒè¯•] vkdb_search - textå€¼: '{tool_params.get('text', 'NOT_FOUND')}', influenceå€¼: '{tool_params.get('influence', 'NOT_FOUND')}'")
        
        try:
            logger.info("ğŸ”„ [èŠ‚ç‚¹æ‰§è¡Œ] vkdb_search - è°ƒç”¨VikingDBæœç´¢å·¥å…·...")
            # è°ƒç”¨æœç´¢å·¥å…· - ç¡®ä¿textå‚æ•°è¢«ä¼ é€’
            tool_result = tool.invoke(tool_params)
            vkdb_response = json.loads(tool_result)
            
            result_count = len(vkdb_response.get("result", {}).get("data", []))
            logger.info(f"âœ… [èŠ‚ç‚¹ç»“æœ] vkdb_search - æœç´¢æˆåŠŸï¼Œè¿”å› {result_count} æ¡ç»“æœ")
            
            # è¯Šæ–­ï¼šåˆ†æVikingDBè¿”å›æ•°æ®çš„ç»“æ„å¤§å°
            if result_count:
                data_list = vkdb_response.get("result", {}).get("data", [])
                # è®¡ç®—æ€»JSONå¤§å°
                total_json_size = len(json.dumps(vkdb_response, ensure_ascii=False))
                logger.info(f"ğŸ“Š [è¯Šæ–­] vkdb_search - VikingDBå“åº”æ€»JSONå¤§å°: {total_json_size:,} å­—ç¬¦")
                
                # åˆ†æç¬¬ä¸€æ¡è®°å½•çš„å­—æ®µå¤§å°
                if data_list and len(data_list) > 0:
                    first_item = data_list[0]
                    logger.info(f"ğŸ“Š [è¯Šæ–­] vkdb_search - ç¬¬ä¸€æ¡è®°å½•çš„å­—æ®µ:")
                    for key, value in first_item.get("fields", {}).items():
                        if isinstance(value, str):
                            field_size = len(value)
                            logger.info(f"   - {key}: {field_size:,} å­—ç¬¦")
                        elif isinstance(value, (dict, list)):
                            field_size = len(json.dumps(value, ensure_ascii=False))
                            logger.info(f"   - {key}: {field_size:,} å­—ç¬¦ (JSON)")
                        else:
                            logger.info(f"   - {key}: {len(str(value))} å­—ç¬¦")
                
                # è®¡ç®—å¹³å‡æ¯æ¡è®°å½•çš„å¤§å°
                avg_item_size = total_json_size / result_count if result_count > 0 else 0
                logger.info(f"ğŸ“Š [è¯Šæ–­] vkdb_search - å¹³å‡æ¯æ¡è®°å½•å¤§å°: {avg_item_size:,.0f} å­—ç¬¦")
                
                # ä»…æ—¥å¿—æ‰“å°å‰5æ¡ï¼Œä¾¿äºæ ¸å¯¹å­—æ®µï¼ˆæˆªæ–­ä»¥é¿å…æ—¥å¿—è¿‡é•¿ï¼‰
                preview = data_list[:5]
                preview_str = json.dumps(preview, ensure_ascii=False)
                logger.info(f"ğŸ“ [èŠ‚ç‚¹ç»“æœ] vkdb_search - å‰5æ¡ç»“æœé¢„è§ˆï¼ˆå‰500å­—ç¬¦ï¼‰: {preview_str[:2000]}...")
            if result_count == 0:
                # æ— ç»“æœæ—¶ç›´æ¥è¿”å›æç¤ºï¼Œåç»­èŠ‚ç‚¹å¯è·³è¿‡
                logger.warning("âš ï¸ [èŠ‚ç‚¹ç»“æœ] vkdb_search - æœªæ”¶å½•è¯¥è¾¾äººï¼Œç›´æ¥è¿”å›æç¤º")
                return {
                    "vkdb_response": vkdb_response,
                    "final_summary": "å½“å‰æœªæ”¶å½•è¯¥è¾¾äºº",
                    "vkdb_influencer": influence_value,
                    "vkdb_no_result": True
                }
            return {
                "vkdb_response": vkdb_response,
                "vkdb_influencer": influence_value,
                "vkdb_no_result": False
            }
        except Exception as e:
            logger.error(f"âŒ [èŠ‚ç‚¹é”™è¯¯] vkdb_search - æœç´¢å¤±è´¥: {e}")
            return {
                "error": f"VikingDB search failed: {e}",
                "vkdb_response": None
            }
    
    return _node


def mysql_join_node(settings: AppSettings):
    """MySQL JoinèŠ‚ç‚¹ï¼šè‡ªåŠ¨ä»VikingDBç»“æœä¸­æå–material_idï¼Œç„¶åJoin MySQL"""
    
    def _node(state: Dict[str, Any]) -> Dict[str, Any]:
        import logging
        logger = logging.getLogger(__name__)
        logger.info("ğŸ” [èŠ‚ç‚¹æ‰§è¡Œ] mysql_join - å¼€å§‹MySQL Join")
        
        # ç›´æ¥è®¿é—®å­—å…¸
        vkdb_response = state.get("vkdb_response")
        if not vkdb_response:
            logger.error("âŒ [èŠ‚ç‚¹é”™è¯¯] mysql_join - æ²¡æœ‰VikingDBæœç´¢ç»“æœ")
            return {"error": "No vkdb_response for MySQL join"}
        if state.get("vkdb_no_result"):
            # ä¸Šæ¸¸å·²åˆ¤å®šæ— æ•°æ®ï¼Œè·³è¿‡Join
            logger.warning("âš ï¸ [èŠ‚ç‚¹ç»“æœ] mysql_join - ä¸Šæ¸¸æ— ç»“æœï¼Œè·³è¿‡Join")
            return {
                "mysql_join_result": None,
                "final_summary": state.get("final_summary"),
                "vkdb_influencer": state.get("vkdb_influencer"),
            }
        
        # ä»VikingDBå“åº”ä¸­æå–å½±å“è€…ä¿¡æ¯
        # å°è¯•ä»vkdb_responseä¸­æå–influencer
        vkdb_result = vkdb_response.get("result", {}).get("data", [])
        influencer = state.get("vkdb_influencer") or state.get("vkdb_query") or "æœªçŸ¥"
        
        # å¦‚æœVikingDBç»“æœä¸­æœ‰influencerå­—æ®µï¼Œä¼˜å…ˆä½¿ç”¨
        if vkdb_result and isinstance(vkdb_result, list) and len(vkdb_result) > 0:
            first_item = vkdb_result[0]
            if isinstance(first_item, dict) and "influencer" in first_item:
                influencer = first_item["influencer"] or influencer
        
        logger.info(f"ğŸ“ [èŠ‚ç‚¹è¾“å…¥] mysql_join - å½±å“è€…: {influencer}")
        
        # ä»settingsè·å–MySQLé…ç½®
        mysql_table = settings.mysql_table
        mysql_max_in = settings.mysql_max_in
        mysql_max_rows = settings.mysql_max_rows
        
        try:
            logger.info("ğŸ”„ [èŠ‚ç‚¹æ‰§è¡Œ] mysql_join - è°ƒç”¨MySQL JoinæœåŠ¡ï¼ˆä½¿ç”¨å·²æœ‰VikingDBç»“æœï¼‰...")
            # ç›´æ¥ä½¿ç”¨stateä¸­çš„vkdb_responseï¼Œé¿å…é‡å¤æœç´¢
            mysql_result = vkdb_response_to_mysql_join(
                vkdb_response=vkdb_response,
                influencer=influencer,
                mysql_max_in=mysql_max_in,
                mysql_table=mysql_table,
                mysql_max_rows=mysql_max_rows,
                require_in=True,
            )
            
            row_count = mysql_result.get("mysql", {}).get("row_count", 0)
            logger.info(f"âœ… [èŠ‚ç‚¹ç»“æœ] mysql_join - JoinæˆåŠŸï¼ŒMySQLè¿”å› {row_count} è¡Œæ•°æ®")
            
            return {
                "mysql_join_result": mysql_result
            }
        except Exception as e:
            logger.error(f"âŒ [èŠ‚ç‚¹é”™è¯¯] mysql_join - Joinå¤±è´¥: {e}")
            return {
                "error": f"MySQL join failed: {e}",
                "mysql_join_result": None
            }
    
    return _node


def llm_summarize_node(settings: AppSettings):
    """LLMæ±‡æ€»èŠ‚ç‚¹ï¼šæ±‡æ€»VikingDBå’ŒMySQLçš„ç»“æœ"""
    
    llm = ChatTongyi(model=settings.qwen_model, temperature=settings.qwen_temperature)
    
    async def _node(state: Dict[str, Any], config: RunnableConfig) -> Dict[str, Any]:
        import logging
        logger = logging.getLogger(__name__)
        logger.info("ğŸ” [èŠ‚ç‚¹æ‰§è¡Œ] llm_summarize - å¼€å§‹LLMæ±‡æ€»")
        
        # ç›´æ¥è®¿é—®å­—å…¸
        vkdb_response = state.get("vkdb_response")
        if not vkdb_response:
            logger.error("âŒ [èŠ‚ç‚¹é”™è¯¯] llm_summarize - æ²¡æœ‰VikingDBæœç´¢ç»“æœ")
            return {"error": "No vkdb_response for summarization"}
        # å¦‚æœä¸Šæ¸¸å·²ç»™å‡ºæœ€ç»ˆæç¤ºï¼ˆå¦‚æœªæ”¶å½•è¾¾äººï¼‰ï¼Œç›´æ¥è¿”å›
        if state.get("final_summary"):
            summary = state.get("final_summary")
            logger.info(f"â„¹ï¸ [èŠ‚ç‚¹ç»“æœ] llm_summarize - ä½¿ç”¨ä¸Šæ¸¸summaryç›´æ¥è¿”å›: {summary}")
            return {"final_summary": summary}
        
        # æ„å»ºæ±‡æ€»æç¤º
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªæ•°æ®åˆ†æåŠ©æ‰‹ã€‚æ ¹æ®VikingDBæœç´¢ç»“æœå’ŒMySQLåˆ†æç»“æœï¼Œä¸ºç”¨æˆ·ç”Ÿæˆæ¸…æ™°ã€æœ‰ç”¨çš„æ€»ç»“ã€‚

é‡ç‚¹å…³æ³¨ï¼š
1. VikingDBæœç´¢åˆ°çš„å†…å®¹
2. MySQLæ•°æ®åˆ†æç»“æœï¼ˆå¦‚æœæœ‰ï¼‰
3. ç”¨æˆ·åŸå§‹æŸ¥è¯¢çš„ç­”æ¡ˆ

ç”¨è‡ªç„¶è¯­è¨€å›å¤ï¼Œä¸è¦è¿”å›JSONã€‚"""
        
        vkdb_info = json.dumps(vkdb_response, ensure_ascii=False, indent=2)
        mysql_info = ""
        mysql_join_result = state.get("mysql_join_result")
        if mysql_join_result:
            mysql_info = f"\n\nMySQLåˆ†æç»“æœï¼š\n{json.dumps(mysql_join_result, ensure_ascii=False, indent=2)}"
        
        user_query = state.get("vkdb_query") or "æŸ¥è¯¢"
        logger.info(f"ğŸ“ [èŠ‚ç‚¹è¾“å…¥] llm_summarize - ç”¨æˆ·æŸ¥è¯¢: {user_query}")
        
        human_prompt = f"""ç”¨æˆ·æŸ¥è¯¢ï¼š{user_query}

VikingDBæœç´¢ç»“æœï¼š
{vkdb_info}
{mysql_info}

è¯·ç”Ÿæˆæ€»ç»“å›å¤ã€‚"""
        
        # è¯Šæ–­ï¼šè¯¦ç»†åˆ†æprompté•¿åº¦ï¼Œå®šä½TTFTå»¶è¿Ÿé—®é¢˜
        system_prompt_len = len(system_prompt)
        human_prompt_len = len(human_prompt)
        vkdb_info_len = len(vkdb_info)
        mysql_info_len = len(mysql_info)
        prompt_length = system_prompt_len + human_prompt_len
        estimated_tokens = prompt_length // 4  # ç²—ç•¥ä¼°ç®—ï¼š1 token â‰ˆ 4 å­—ç¬¦
        
        logger.info(f"ğŸ“Š [èŠ‚ç‚¹è¯Šæ–­] llm_summarize - Promptç»„æˆåˆ†æ:")
        logger.info(f"   - System Prompt: {system_prompt_len:,} å­—ç¬¦")
        logger.info(f"   - Human Prompt: {human_prompt_len:,} å­—ç¬¦")
        logger.info(f"     * ç”¨æˆ·æŸ¥è¯¢éƒ¨åˆ†: {len(user_query)} å­—ç¬¦")
        logger.info(f"     * VikingDBç»“æœ: {vkdb_info_len:,} å­—ç¬¦ ({vkdb_info_len/prompt_length*100:.1f}% of prompt)")
        logger.info(f"     * MySQLç»“æœ: {mysql_info_len:,} å­—ç¬¦")
        logger.info(f"   - Promptæ€»é•¿åº¦: {prompt_length:,} å­—ç¬¦")
        logger.info(f"   - ä¼°ç®—Tokenæ•°: ~{estimated_tokens:,} tokens")
        
        # å¦‚æœpromptå¾ˆå¤§ï¼Œç»™å‡ºè­¦å‘Š
        if estimated_tokens > 100000:
            logger.warning(f"âš ï¸ [èŠ‚ç‚¹è¯Šæ–­] llm_summarize - Promptéå¸¸å¤§ (~{estimated_tokens:,} tokens)ï¼Œå¯èƒ½å¯¼è‡´TTFTå»¶è¿Ÿ")
        elif estimated_tokens > 50000:
            logger.warning(f"âš ï¸ [èŠ‚ç‚¹è¯Šæ–­] llm_summarize - Promptè¾ƒå¤§ (~{estimated_tokens:,} tokens)ï¼Œå¯èƒ½å½±å“TTFT")
        
        try:
            import time
            prefill_start_time = time.time()
            logger.info("ğŸ”„ [èŠ‚ç‚¹æ‰§è¡Œ] llm_summarize - å¼€å§‹è°ƒç”¨LLM APIï¼ˆå³å°†è¿›å…¥Prefillé˜¶æ®µï¼‰...")
            logger.info(f"â±ï¸ [æ—¶é—´è¯Šæ–­] llm_summarize - APIè°ƒç”¨å¼€å§‹æ—¶é—´: {prefill_start_time:.3f}")
            summary_chunks: list[str] = []
            # å…³é”®ä¿®å¤ï¼šä¼ é€’ config ç»™ astreamï¼Œè®©æµå¼äº‹ä»¶èƒ½è¢« astream_events æ•è·
            async_iterator = llm.astream([
                SystemMessage(content=system_prompt),
                HumanMessage(content=human_prompt)
            ], config=config)
            
            # è®°å½•ç¬¬ä¸€ä¸ªchunkåˆ°è¾¾çš„æ—¶é—´
            first_chunk_time = None
            async for chunk in async_iterator:
                if first_chunk_time is None:
                    first_chunk_time = time.time()
                    ttft = first_chunk_time - prefill_start_time
                    logger.info(f"â±ï¸ [æ—¶é—´è¯Šæ–­] llm_summarize - ç¬¬ä¸€ä¸ªTokenåˆ°è¾¾æ—¶é—´: {first_chunk_time:.3f}")
                    logger.info(f"â±ï¸ [æ—¶é—´è¯Šæ–­] llm_summarize - TTFT (Time To First Token): {ttft:.2f} ç§’")
                if hasattr(chunk, "content") and chunk.content:
                    summary_chunks.append(chunk.content)
            # æ¶ˆè´¹æµå¹¶ç´¯ç§¯å†…å®¹ï¼Œäº‹ä»¶å±‚ä¼šç›´æ¥æŠŠchunkæ¨ç»™å‰ç«¯
            async for chunk in async_iterator:
                if hasattr(chunk, "content") and chunk.content:
                    summary_chunks.append(chunk.content)
            summary = "".join(summary_chunks)
            logger.info(f"âœ… [èŠ‚ç‚¹ç»“æœ] llm_summarize - æ±‡æ€»å®Œæˆï¼Œé•¿åº¦: {len(summary)} å­—ç¬¦")
            return {
                "final_summary": summary
            }
        except Exception as e:
            logger.error(f"âŒ [èŠ‚ç‚¹é”™è¯¯] llm_summarize - æ±‡æ€»å¤±è´¥: {e}")
            return {
                "error": f"LLM summarization failed: {e}",
                "final_summary": None
            }
    
    return _node


def simple_chat_node(settings: AppSettings):
    """ç®€å•èŠå¤©èŠ‚ç‚¹ï¼šç›´æ¥LLMå›å¤"""
    
    llm = ChatTongyi(model=settings.qwen_model, temperature=settings.qwen_temperature)
    
    def _node(state: Dict[str, Any], config: RunnableConfig) -> Dict[str, Any]:
        import logging
        logger = logging.getLogger(__name__)
        logger.info("ğŸ” [èŠ‚ç‚¹æ‰§è¡Œ] simple_chat - å¼€å§‹ç®€å•èŠå¤©")
        
        # ç›´æ¥è®¿é—®å­—å…¸
        messages = state.get("messages", [])
        
        if not messages:
            logger.error("âŒ [èŠ‚ç‚¹é”™è¯¯] simple_chat - æ²¡æœ‰æ¶ˆæ¯")
            return {"error": "No messages provided"}
        
        user_input = messages[-1].content if messages else ""
        logger.info(f"ğŸ“ [èŠ‚ç‚¹è¾“å…¥] simple_chat - ç”¨æˆ·è¾“å…¥: {user_input[:50]}...")
        
        try:
            logger.info("ğŸ”„ [èŠ‚ç‚¹æ‰§è¡Œ] simple_chat - è°ƒç”¨LLMç”Ÿæˆå›å¤...")
            # ä¼ é€’ config ç»™ invokeï¼Œç¡®ä¿äº‹ä»¶èƒ½è¢«æ•è·ï¼ˆè™½ç„¶å½“å‰æ˜¯ invokeï¼Œä½†ä¸ºç»Ÿä¸€æ€§åŠ ä¸Šï¼‰
            response = llm.invoke(messages, config=config)
            content = response.content if hasattr(response, 'content') else str(response)
            
            logger.info(f"âœ… [èŠ‚ç‚¹ç»“æœ] simple_chat - å›å¤ç”Ÿæˆå®Œæˆï¼Œé•¿åº¦: {len(content)} å­—ç¬¦")
            
            return {
                "final_summary": content
            }
        except Exception as e:
            logger.error(f"âŒ [èŠ‚ç‚¹é”™è¯¯] simple_chat - èŠå¤©å¤±è´¥: {e}")
            return {
                "error": f"Simple chat failed: {e}",
                "final_summary": None
            }
    
    return _node
