from __future__ import annotations

import json
import re
from datetime import datetime, date
from typing import Any, Dict, Optional, List as TypingList

from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.runnables import RunnableConfig
from langchain_community.chat_models import ChatTongyi
from pydantic import BaseModel, Field


class DateTimeJSONEncoder(json.JSONEncoder):
    """è‡ªå®šä¹‰JSONç¼–ç å™¨ï¼Œå¤„ç†datetimeå’Œdateå¯¹è±¡"""
    def default(self, obj):
        if isinstance(obj, (datetime, date)):
            return obj.isoformat()
        return super().default(obj)

from src.core.settings import AppSettings
from src.domain.state import FrontendSearchInput
from src.graphs.agent_graph.tools import make_vkdb_search_tool
from src.services.vkdb_mysql_service import vkdb_response_to_mysql_join
from src.services.intent_structurize_service import structurize_intents_batch
from src.utils.data_aggregator import generate_aggregation_csv


class DateTimeJSONEncoder(json.JSONEncoder):
    """è‡ªå®šä¹‰JSONç¼–ç å™¨ï¼Œå¤„ç†datetimeå’Œdateå¯¹è±¡"""
    def default(self, obj):
        if isinstance(obj, (datetime, date)):
            return obj.isoformat()
        return super().default(obj)


def _generate_chart_image_markdown(plot_data: List[Dict[str, Any]]) -> str:
    """
    ç”Ÿæˆå›¾è¡¨å›¾ç‰‡å¹¶è¿”å› Markdown æ ¼å¼å­—ç¬¦ä¸²ï¼ˆä½¿ç”¨ QuickChart æœåŠ¡ï¼‰
    
    Args:
        plot_data: LLMåˆ†æç»“æœä¸­çš„plot_data
    
    Returns:
        str: Markdown æ ¼å¼çš„å›¾ç‰‡å­—ç¬¦ä¸²ï¼Œå¦‚æœå¤±è´¥è¿”å›ç©ºå­—ç¬¦ä¸²
    """
    if not plot_data:
        return ""
    
    try:
        import json
        import urllib.parse
        
        # æå–æ•°æ®
        categories = [item.get("category", "Unknown") for item in plot_data]
        roi_values = [float(item.get("roi", 0.0)) for item in plot_data]
        ctr_values = [float(item.get("ctr", 0.0)) for item in plot_data]
        
        # æ„å»º QuickChart çš„é…ç½® JSON
        # ä½¿ç”¨ç»„åˆå›¾è¡¨ï¼šæŸ±çŠ¶å›¾æ˜¾ç¤º ROIï¼ŒæŠ˜çº¿å›¾æ˜¾ç¤º CTR
        chart_config = {
            "type": "bar",
            "data": {
                "labels": categories,
                "datasets": [
                    {
                        "label": "ROI",
                        "type": "bar",
                        "data": roi_values,
                        "backgroundColor": [
                            "rgba(255, 107, 107, 0.7)" if x > 20 else "rgba(78, 205, 196, 0.7)"
                            for x in roi_values
                        ],
                        "borderColor": [
                            "rgba(255, 107, 107, 1)" if x > 20 else "rgba(78, 205, 196, 1)"
                            for x in roi_values
                        ],
                        "borderWidth": 1,
                        "yAxisID": "y"
                    },
                    {
                        "label": "CTR",
                        "type": "line",
                        "data": ctr_values,
                        "backgroundColor": "rgba(255, 165, 0, 0.2)",
                        "borderColor": "rgba(255, 165, 0, 1)",
                        "borderWidth": 2,
                        "fill": False,
                        "pointRadius": 4,
                        "pointBackgroundColor": "rgba(255, 165, 0, 1)",
                        "yAxisID": "y1"
                    }
                ]
            },
            "options": {
                "responsive": True,
                "plugins": {
                    "title": {
                        "display": True,
                        "text": "Strategy Effectiveness Analysis",
                        "font": {"size": 16, "weight": "bold"}
                    },
                    "legend": {
                        "display": True,
                        "position": "top"
                    }
                },
                "scales": {
                    "y": {
                        "type": "linear",
                        "display": True,
                        "position": "left",
                        "title": {
                            "display": True,
                            "text": "ROI"
                        }
                    },
                    "y1": {
                        "type": "linear",
                        "display": True,
                        "position": "right",
                        "title": {
                            "display": True,
                            "text": "CTR"
                        },
                        "grid": {
                            "drawOnChartArea": False
                        }
                    },
                    "x": {
                        "ticks": {
                            "maxRotation": 45,
                            "minRotation": 45
                        }
                    }
                }
            }
        }
        
        # å°†é…ç½®è½¬æ¢ä¸º JSON å­—ç¬¦ä¸²å¹¶å‹ç¼©ï¼ˆå»é™¤ç©ºæ ¼ï¼‰
        chart_config_json = json.dumps(chart_config, separators=(',', ':'))
        
        # ç”Ÿæˆ QuickChart URLï¼ˆæŒ‡å®šä½¿ç”¨ Chart.js v3ï¼‰
        base_url = "https://quickchart.io/chart"
        chart_url = f"{base_url}?v=3&c={urllib.parse.quote(chart_config_json)}&w=800&h=400&format=png"
        
        # è¿”å› Markdown æ ¼å¼
        return f"\n\n![Analysis Chart]({chart_url})"
        
    except Exception as e:
        import logging
        logger = logging.getLogger(__name__)
        logger.warning(f"âš ï¸ [å›¾è¡¨ç”Ÿæˆ] ç”Ÿæˆå›¾è¡¨å¤±è´¥: {e}")
        return ""


def intent_analysis_node(settings: AppSettings):
    """æ„å›¾åˆ†æèŠ‚ç‚¹ï¼šåˆ¤æ–­æ˜¯ç®€å•èŠå¤©è¿˜æ˜¯éœ€è¦VikingDBæœç´¢"""
    
    llm = ChatTongyi(model=settings.qwen_model, temperature=settings.qwen_temperature)
    
    def _node(state: Dict[str, Any]) -> Dict[str, Any]:
        import logging
        logger = logging.getLogger(__name__)
        logger.info("ğŸ” [èŠ‚ç‚¹æ‰§è¡Œ] intent_analysis - å¼€å§‹æ„å›¾åˆ†æ")
        
        # ç›´æ¥è®¿é—®å­—å…¸ï¼ŒMessagesStateæ˜¯TypedDict
        messages = state.get("messages", [])
        last_message = messages[-1] if messages else None
        
        if not last_message:
            logger.error("âŒ [èŠ‚ç‚¹é”™è¯¯] intent_analysis - æ²¡æœ‰æ¶ˆæ¯")
            return {"error": "No message provided"}
        
        user_input = last_message.content
        logger.info(f"ğŸ“ [èŠ‚ç‚¹è¾“å…¥] intent_analysis - ç”¨æˆ·è¾“å…¥: {user_input[:50]}...")
        
        # ä½¿ç”¨LLMåˆ¤æ–­æ„å›¾ï¼Œå¹¶å°½é‡æå– influencer åç§°
        system_prompt = """ä½ æ˜¯æ„å›¾åˆ†æåŠ©æ‰‹ï¼Œå¿…é¡»è¿”å› JSONã€‚

è§„åˆ™ï¼š
- å¦‚æœç”¨æˆ·åœ¨æ‰¾è§†é¢‘/æ•°æ®/å½±å“è€…/æœç´¢/æŸ¥è¯¢/æŸ¥æ‰¾/åˆ†æï¼Œæ„å›¾ä¸º vkdb_searchã€‚
- å¦‚æœç”¨æˆ·åªæ˜¯é—²èŠï¼Œæ„å›¾ä¸º simple_chatã€‚

è¾“å‡º JSON ç»“æ„ï¼ˆå­—æ®µåå›ºå®šï¼Œå…¨éƒ¨å°å†™ï¼‰ï¼š
{
  "intent": "vkdb_search" | "simple_chat",
  "query": "å°½é‡æå–çš„æ ¸å¿ƒæŸ¥è¯¢è¯ï¼Œå¦‚ï¼šæè¯çš„è§†é¢‘",    // vkdb_search å¿…å¡«
  "influencer": "å½±å“è€…å§“åï¼Œå¦‚æœèƒ½ç¡®å®šå°±å¡«ï¼Œæ¯”å¦‚ï¼šæè¯ï¼›å¦åˆ™ç•™ç©º" // å¯é€‰
}

æ³¨æ„ï¼š
- åªè¾“å‡º JSONï¼Œä¸è¦å…¶å®ƒå†…å®¹ã€‚
- å¦‚æœèƒ½ç¡®å®š influencer å°±å¡«å…·ä½“å§“åï¼Œå¦åˆ™å¡«ç©ºå­—ç¬¦ä¸²ã€‚
"""
        
        analysis_prompt = f"ç”¨æˆ·è¾“å…¥ï¼š{user_input}\n\nè¯·æŒ‰ç…§ä¸Šè¿° JSON ç»“æ„è¿”å›ã€‚"
        
        try:
            # ä½¿ç”¨ç»“æ„åŒ–è¾“å‡º
            class IntentResult(BaseModel):
                intent: str
                query: Optional[str] = None
                influencer: Optional[str] = None
            
            structured_llm = llm.with_structured_output(IntentResult)
            result = structured_llm.invoke([
                SystemMessage(content=system_prompt),
                HumanMessage(content=analysis_prompt)
            ])
            
            if result.intent == "vkdb_search":
                # ä¼˜å…ˆä½¿ç”¨è§£æå‡ºçš„ influencerï¼Œå¦åˆ™ç”¨ queryï¼Œå¦åˆ™å›é€€ user_input
                influencer = (result.influencer or "").strip()
                query = (result.query or influencer or user_input).strip()
                logger.info(f"âœ… [èŠ‚ç‚¹ç»“æœ] intent_analysis - æ„å›¾: vkdb_search, æŸ¥è¯¢: {query}, influencer: {influencer}")
                return {
                    "intent": "vkdb_search",
                    "vkdb_query": query,
                    "vkdb_influencer": influencer or None
                }
            else:
                logger.info(f"âœ… [èŠ‚ç‚¹ç»“æœ] intent_analysis - æ„å›¾: simple_chat")
                return {
                    "intent": "simple_chat"
                }
        except Exception as e:
            logger.warning(f"âš ï¸ [èŠ‚ç‚¹è­¦å‘Š] intent_analysis - ç»“æ„åŒ–è¾“å‡ºå¤±è´¥ï¼Œä½¿ç”¨fallback: {e}")
            # å¦‚æœç»“æ„åŒ–è¾“å‡ºå¤±è´¥ï¼Œä½¿ç”¨ç®€å•è§„åˆ™ä½œä¸ºfallback
            vkdb_keywords = ["æœç´¢", "æŸ¥æ‰¾", "æŸ¥è¯¢", "æ‰¾", "æŸ¥", "æ•°æ®", "è§†é¢‘", "å½±å“è€…", "åˆ†æ"]
            if any(keyword in user_input for keyword in vkdb_keywords):
                logger.info(f"âœ… [èŠ‚ç‚¹ç»“æœ] intent_analysis - æ„å›¾: vkdb_search (fallback)")
                return {
                    "intent": "vkdb_search",
                    "vkdb_query": user_input
                }
            logger.info(f"âœ… [èŠ‚ç‚¹ç»“æœ] intent_analysis - æ„å›¾: simple_chat (fallback)")
            return {
                "intent": "simple_chat"
            }
    
    return _node


def vkdb_search_node(settings: AppSettings):
    """VikingDBæœç´¢èŠ‚ç‚¹"""
    
    # æ ¹æ®é…ç½®é€‰æ‹©æ£€ç´¢æ–¹å¼
    search_method = getattr(settings, 'vikingdb_search_method', 'multi_modal')
    
    if search_method == "random":
        from src.graphs.agent_graph.tools import make_vkdb_random_search_tool
        tool = make_vkdb_random_search_tool(settings)
    else:
        from src.graphs.agent_graph.tools import make_vkdb_search_tool
        tool = make_vkdb_search_tool(settings)
    
    def _node(state: Dict[str, Any]) -> Dict[str, Any]:
        import logging
        logger = logging.getLogger(__name__)
        logger.info(f"ğŸ” [èŠ‚ç‚¹æ‰§è¡Œ] vkdb_search - å¼€å§‹VikingDBæœç´¢ (æ¨¡å¼: {search_method})")
        
        # ç›´æ¥è®¿é—®å­—å…¸
        intent = state.get("intent")
        if intent != "vkdb_search":
            logger.error(f"âŒ [èŠ‚ç‚¹é”™è¯¯] vkdb_search - æ— æ•ˆæ„å›¾: {intent}")
            return {"error": "Invalid intent for vkdb_search node"}
        
        query = state.get("vkdb_query") or ""
        influencer_hint = (state.get("vkdb_influencer") or "").strip()
        
        # éšæœºæ£€ç´¢ä¸éœ€è¦queryï¼Œåªéœ€è¦influencerï¼ˆå¦‚æœæ²¡æœ‰influencerï¼Œä½¿ç”¨queryä½œä¸ºfallbackï¼‰
        if search_method == "random":
            if not influencer_hint and not query:
                logger.error("âŒ [èŠ‚ç‚¹é”™è¯¯] vkdb_search - éšæœºæ£€ç´¢éœ€è¦influenceræˆ–query")
                return {"error": "Random search requires influencer or query"}
        else:
            if not query:
                logger.error("âŒ [èŠ‚ç‚¹é”™è¯¯] vkdb_search - æ²¡æœ‰æŸ¥è¯¢å†…å®¹")
                return {"error": "No query provided"}
        
        logger.info(f"ğŸ“ [èŠ‚ç‚¹è¾“å…¥] vkdb_search - æŸ¥è¯¢: {query}")
        if influencer_hint:
            logger.info(f"ğŸ“Œ [èŠ‚ç‚¹è¾“å…¥] vkdb_search - è§£æå‡ºçš„influencer: {influencer_hint}")
        
        # ä»settingsè¯»å–é…ç½®
        vkdb_limit = settings.vikingdb_default_limit
        # è§£æè¾“å‡ºå­—æ®µï¼ˆä»ç¯å¢ƒå˜é‡æˆ–é»˜è®¤å€¼ï¼‰
        # Agent Graph éœ€è¦çš„å­—æ®µï¼šinfluencer, intent_analysis, landscape_video
        # ä¼˜å…ˆä½¿ç”¨ VKDB_AGENT_OUTPUT_FIELDSï¼Œå¦‚æœæ²¡æœ‰è®¾ç½®åˆ™ä½¿ç”¨é»˜è®¤å¿…éœ€å­—æ®µ
        if settings.vkdb_output_fields:
            from src.infra.vkdb.client import parse_output_fields
            output_fields = parse_output_fields(settings.vkdb_output_fields)
        else:
            # é»˜è®¤ä½¿ç”¨Agent Graphå¿…éœ€çš„å­—æ®µï¼ˆè¿™äº›å­—æ®µæ˜¯Agent Graphå·¥ä½œæµå¿…éœ€çš„ï¼‰
            # æ³¨æ„ï¼šå¦‚æœéœ€è¦ä¿®æ”¹è¿™äº›é»˜è®¤å­—æ®µï¼Œè¯·åœ¨ .env ä¸­è®¾ç½® VKDB_AGENT_OUTPUT_FIELDS
            output_fields = ["influencer", "intent_analysis", "landscape_video"]
        
        # æ ¹æ®æ£€ç´¢æ–¹å¼æ„å»ºä¸åŒçš„è¾“å…¥å‚æ•°
        influence_value = influencer_hint or query  # ä¼˜å…ˆä½¿ç”¨æ„å›¾è§£æå‡ºçš„ influencer
        
        if search_method == "random":
            # éšæœºæ£€ç´¢åªéœ€è¦influenceï¼Œä¸éœ€è¦text
            search_input = FrontendSearchInput(
                influence=influence_value,
                limit=vkdb_limit,
                output_fields=output_fields
            )
            logger.info(f"ğŸ“‹ [èŠ‚ç‚¹å‚æ•°] vkdb_search - influencer: {influence_value}, limit: {vkdb_limit}, output_fields: {search_input.output_fields}")
        else:
            # å¤šæ¨¡æ€æ£€ç´¢éœ€è¦text
            search_input = FrontendSearchInput(
                influence=influence_value,
                text=query,  # å¤šæ¨¡æ€éœ€è¦text
                limit=vkdb_limit,
                output_fields=output_fields
            )
            logger.info(f"ğŸ“‹ [èŠ‚ç‚¹å‚æ•°] vkdb_search - influencer: {influence_value}, text: {query}, limit: {vkdb_limit}, output_fields: {search_input.output_fields}")
        
        # è°ƒè¯•ï¼šæŸ¥çœ‹å®é™…ä¼ é€’ç»™å·¥å…·çš„å‚æ•°
        tool_params = search_input.model_dump(exclude_none=True)
        logger.info(f"ğŸ” [èŠ‚ç‚¹è°ƒè¯•] vkdb_search - å·¥å…·å‚æ•°: {tool_params}")
        if search_method != "random":
            logger.info(f"ğŸ” [èŠ‚ç‚¹è°ƒè¯•] vkdb_search - textå€¼: '{tool_params.get('text', 'NOT_FOUND')}', influenceå€¼: '{tool_params.get('influence', 'NOT_FOUND')}'")
        else:
            logger.info(f"ğŸ” [èŠ‚ç‚¹è°ƒè¯•] vkdb_search - influenceå€¼: '{tool_params.get('influence', 'NOT_FOUND')}'")
        
        try:
            logger.info("ğŸ”„ [èŠ‚ç‚¹æ‰§è¡Œ] vkdb_search - è°ƒç”¨VikingDBæœç´¢å·¥å…·...")
            # è°ƒç”¨æœç´¢å·¥å…· - ç¡®ä¿textå‚æ•°è¢«ä¼ é€’
            tool_result = tool.invoke(tool_params)
            vkdb_response = json.loads(tool_result)
            
            result_count = len(vkdb_response.get("result", {}).get("data", []))
            logger.info(f"âœ… [èŠ‚ç‚¹ç»“æœ] vkdb_search - æœç´¢æˆåŠŸï¼Œè¿”å› {result_count} æ¡ç»“æœ")
            
            # è¯Šæ–­ï¼šåˆ†æVikingDBè¿”å›æ•°æ®çš„ç»“æ„å¤§å°
            if result_count:
                data_list = vkdb_response.get("result", {}).get("data", [])
                # è®¡ç®—æ€»JSONå¤§å°
                total_json_size = len(json.dumps(vkdb_response, ensure_ascii=False))
                logger.info(f"ğŸ“Š [è¯Šæ–­] vkdb_search - VikingDBå“åº”æ€»JSONå¤§å°: {total_json_size:,} å­—ç¬¦")
                
                # åˆ†æç¬¬ä¸€æ¡è®°å½•çš„å­—æ®µå¤§å°
                if data_list and len(data_list) > 0:
                    first_item = data_list[0]
                    logger.info(f"ğŸ“Š [è¯Šæ–­] vkdb_search - ç¬¬ä¸€æ¡è®°å½•çš„å­—æ®µ:")
                    for key, value in first_item.get("fields", {}).items():
                        if isinstance(value, str):
                            field_size = len(value)
                            logger.info(f"   - {key}: {field_size:,} å­—ç¬¦")
                        elif isinstance(value, (dict, list)):
                            field_size = len(json.dumps(value, ensure_ascii=False))
                            logger.info(f"   - {key}: {field_size:,} å­—ç¬¦ (JSON)")
                        else:
                            logger.info(f"   - {key}: {len(str(value))} å­—ç¬¦")
                
                # è®¡ç®—å¹³å‡æ¯æ¡è®°å½•çš„å¤§å°
                avg_item_size = total_json_size / result_count if result_count > 0 else 0
                logger.info(f"ğŸ“Š [è¯Šæ–­] vkdb_search - å¹³å‡æ¯æ¡è®°å½•å¤§å°: {avg_item_size:,.0f} å­—ç¬¦")
                
                # ä»…æ—¥å¿—æ‰“å°å‰5æ¡ï¼Œä¾¿äºæ ¸å¯¹å­—æ®µï¼ˆæˆªæ–­ä»¥é¿å…æ—¥å¿—è¿‡é•¿ï¼‰
                preview = data_list[:5]
                preview_str = json.dumps(preview, ensure_ascii=False)
                logger.info(f"ğŸ“ [èŠ‚ç‚¹ç»“æœ] vkdb_search - å‰5æ¡ç»“æœé¢„è§ˆï¼ˆå‰500å­—ç¬¦ï¼‰: {preview_str[:2000]}...")
            if result_count == 0:
                # æ— ç»“æœæ—¶ç›´æ¥è¿”å›æç¤ºï¼Œåç»­èŠ‚ç‚¹å¯è·³è¿‡
                logger.warning("âš ï¸ [èŠ‚ç‚¹ç»“æœ] vkdb_search - æœªæ”¶å½•è¯¥è¾¾äººï¼Œç›´æ¥è¿”å›æç¤º")
                return {
                    "vkdb_response": vkdb_response,
                    "final_summary": "å½“å‰æœªæ”¶å½•è¯¥è¾¾äºº",
                    "vkdb_influencer": influence_value,
                    "vkdb_no_result": True
                }
            return {
                "vkdb_response": vkdb_response,
                "vkdb_influencer": influence_value,
                "vkdb_no_result": False
            }
        except Exception as e:
            logger.error(f"âŒ [èŠ‚ç‚¹é”™è¯¯] vkdb_search - æœç´¢å¤±è´¥: {e}")
            return {
                "error": f"VikingDB search failed: {e}",
                "vkdb_response": None
            }
    
    return _node


def mysql_join_node(settings: AppSettings):
    """MySQL JoinèŠ‚ç‚¹ï¼šè‡ªåŠ¨ä»VikingDBç»“æœä¸­æå–material_idï¼Œç„¶åJoin MySQL"""
    
    def _node(state: Dict[str, Any]) -> Dict[str, Any]:
        import logging
        logger = logging.getLogger(__name__)
        logger.info("ğŸ” [èŠ‚ç‚¹æ‰§è¡Œ] mysql_join - å¼€å§‹MySQL Join")
        
        # ç›´æ¥è®¿é—®å­—å…¸
        vkdb_response = state.get("vkdb_response")
        if not vkdb_response:
            logger.error("âŒ [èŠ‚ç‚¹é”™è¯¯] mysql_join - æ²¡æœ‰VikingDBæœç´¢ç»“æœ")
            return {"error": "No vkdb_response for MySQL join"}
        if state.get("vkdb_no_result"):
            # ä¸Šæ¸¸å·²åˆ¤å®šæ— æ•°æ®ï¼Œè·³è¿‡Join
            logger.warning("âš ï¸ [èŠ‚ç‚¹ç»“æœ] mysql_join - ä¸Šæ¸¸æ— ç»“æœï¼Œè·³è¿‡Join")
            return {
                "mysql_join_result": None,
                "final_summary": state.get("final_summary"),
                "vkdb_influencer": state.get("vkdb_influencer"),
            }
        
        # ä»VikingDBå“åº”ä¸­æå–å½±å“è€…ä¿¡æ¯
        # å°è¯•ä»vkdb_responseä¸­æå–influencer
        vkdb_result = vkdb_response.get("result", {}).get("data", [])
        influencer = state.get("vkdb_influencer") or state.get("vkdb_query") or "æœªçŸ¥"
        
        # å¦‚æœVikingDBç»“æœä¸­æœ‰influencerå­—æ®µï¼Œä¼˜å…ˆä½¿ç”¨
        if vkdb_result and isinstance(vkdb_result, list) and len(vkdb_result) > 0:
            first_item = vkdb_result[0]
            if isinstance(first_item, dict) and "influencer" in first_item:
                influencer = first_item["influencer"] or influencer
        
        logger.info(f"ğŸ“ [èŠ‚ç‚¹è¾“å…¥] mysql_join - å½±å“è€…: {influencer}")
        
        # ä»settingsè·å–MySQLé…ç½®
        mysql_table = settings.mysql_table
        mysql_max_in = settings.mysql_max_in
        mysql_max_rows = settings.mysql_max_rows
        
        try:
            logger.info("ğŸ”„ [èŠ‚ç‚¹æ‰§è¡Œ] mysql_join - è°ƒç”¨MySQL JoinæœåŠ¡ï¼ˆä½¿ç”¨å·²æœ‰VikingDBç»“æœï¼‰...")
            # ç›´æ¥ä½¿ç”¨stateä¸­çš„vkdb_responseï¼Œé¿å…é‡å¤æœç´¢
            mysql_result = vkdb_response_to_mysql_join(
                vkdb_response=vkdb_response,
                influencer=influencer,
                mysql_max_in=mysql_max_in,
                mysql_table=mysql_table,
                mysql_max_rows=mysql_max_rows,
                require_in=True,
            )
            
            row_count = mysql_result.get("mysql", {}).get("row_count", 0)
            material_ids_count = len(mysql_result.get("vkdb", {}).get("material_ids", []))
            logger.info(f"âœ… [èŠ‚ç‚¹ç»“æœ] mysql_join - JoinæˆåŠŸï¼Œæå–äº† {material_ids_count} ä¸ªmaterialIdï¼ŒMySQLè¿”å› {row_count} è¡Œæ•°æ®")
            
            # å¦‚æœmaterialIdæœ‰ä½†MySQLè¿”å›0è¡Œï¼Œè®°å½•è­¦å‘Š
            if material_ids_count > 0 and row_count == 0:
                material_ids = mysql_result.get("vkdb", {}).get("material_ids", [])[:5]  # åªæ˜¾ç¤ºå‰5ä¸ª
                logger.warning(f"âš ï¸ [èŠ‚ç‚¹è­¦å‘Š] mysql_join - æœ‰ {material_ids_count} ä¸ªmaterialIdä½†MySQLè¿”å›0è¡Œï¼Œå¯èƒ½åŸå› ï¼š")
                logger.warning(f"   1. roi2MaterialVideoNameå­—æ®µä¸åŒ…å«'{influencer}'")
                logger.warning(f"   2. ä¸æ»¡è¶³liveShowCountForRoi2V2 > 0æˆ–liveWatchCountForRoi2V2 > 0æ¡ä»¶")
                logger.warning(f"   3. materialIdåœ¨MySQLä¸­ä¸å­˜åœ¨")
                logger.warning(f"   ç¤ºä¾‹materialId: {material_ids}")
            
            return {
                "mysql_join_result": mysql_result
            }
        except Exception as e:
            logger.error(f"âŒ [èŠ‚ç‚¹é”™è¯¯] mysql_join - Joinå¤±è´¥: {e}")
            return {
                "error": f"MySQL join failed: {e}",
                "mysql_join_result": None
            }
    
    return _node


def llm_summarize_node(settings: AppSettings):
    """LLMæ±‡æ€»èŠ‚ç‚¹ï¼šæ±‡æ€»VikingDBå’ŒMySQLçš„ç»“æœï¼Œæ•´åˆåˆ†ææ´å¯Ÿï¼Œç”Ÿæˆå›¾è¡¨"""
    
    llm = ChatTongyi(model=settings.qwen_model, temperature=settings.qwen_temperature)
    
    async def _node(state: Dict[str, Any], config: RunnableConfig) -> Dict[str, Any]:
        import logging
        logger = logging.getLogger(__name__)
        logger.info("ğŸ” [èŠ‚ç‚¹æ‰§è¡Œ] llm_summarize - å¼€å§‹LLMæ±‡æ€»")
        
        # ç›´æ¥è®¿é—®å­—å…¸
        vkdb_response = state.get("vkdb_response")
        if not vkdb_response:
            logger.error("âŒ [èŠ‚ç‚¹é”™è¯¯] llm_summarize - æ²¡æœ‰VikingDBæœç´¢ç»“æœ")
            return {"error": "No vkdb_response for summarization"}
        # å¦‚æœä¸Šæ¸¸å·²ç»™å‡ºæœ€ç»ˆæç¤ºï¼ˆå¦‚æœªæ”¶å½•è¾¾äººï¼‰ï¼Œç›´æ¥è¿”å›
        if state.get("final_summary"):
            summary = state.get("final_summary")
            logger.info(f"â„¹ï¸ [èŠ‚ç‚¹ç»“æœ] llm_summarize - ä½¿ç”¨ä¸Šæ¸¸summaryç›´æ¥è¿”å›: {summary}")
            return {"final_summary": summary}
        
        # æ£€æŸ¥æ˜¯å¦æœ‰åˆ†æç»“æœï¼ˆæ–°æµç¨‹ï¼‰
        analysis_result = state.get("analysis_result")
        has_analysis = analysis_result is not None and analysis_result.get("summary") is not None
        
        if has_analysis:
            # æ–°æµç¨‹ï¼šæ•´åˆåˆ†ææ´å¯Ÿ
            logger.info("ğŸ“Š [èŠ‚ç‚¹] llm_summarize - æ£€æµ‹åˆ°åˆ†æç»“æœï¼Œä½¿ç”¨æ–°æµç¨‹")
            
            # æ„å»ºæ±‡æ€»æç¤ºï¼ˆæ•´åˆåˆ†ææ´å¯Ÿï¼‰
            system_prompt = """ä½ æ˜¯ä¸€ä¸ªæ•°æ®åˆ†æåŠ©æ‰‹ã€‚æ ¹æ®AIåˆ†æå¸ˆçš„æ´å¯Ÿå’ŒåŸå§‹æ•°æ®ï¼Œä¸ºç”¨æˆ·ç”Ÿæˆæ¸…æ™°ã€æœ‰ç”¨çš„æ€»ç»“ã€‚

é‡ç‚¹å…³æ³¨ï¼š
1. AIåˆ†æå¸ˆçš„æ·±åº¦æ´å¯Ÿï¼ˆkey_insightï¼‰
2. é»„é‡‘æ³•åˆ™ï¼ˆgolden_ruleï¼‰
3. ç”¨æˆ·åŸå§‹æŸ¥è¯¢çš„ç­”æ¡ˆ

ç”¨è‡ªç„¶è¯­è¨€å›å¤ï¼Œä¸è¦è¿”å›JSONã€‚"""
            
            key_insight = analysis_result.get("summary", {}).get("key_insight", "")
            golden_rule = analysis_result.get("summary", {}).get("golden_rule", "")
            
            user_query = state.get("vkdb_query") or "æŸ¥è¯¢"
            logger.info(f"ğŸ“ [èŠ‚ç‚¹è¾“å…¥] llm_summarize - ç”¨æˆ·æŸ¥è¯¢: {user_query}")
            
            # è®°å½•å®Œæ•´çš„åˆ†æç»“æœä¾›è¯Šæ–­
            plot_data = analysis_result.get("plot_data", [])
            logger.info(f"ğŸ“Š [æ•°æ®è¯¦æƒ…] llm_summarize - æ¥æ”¶åˆ°çš„åˆ†æç»“æœ:")
            logger.info(f"   - key_insight é•¿åº¦: {len(key_insight)} å­—ç¬¦")
            logger.info(f"   - key_insight å†…å®¹é¢„è§ˆ: {key_insight[:500]}...")
            logger.info(f"   - golden_rule: {golden_rule}")
            logger.info(f"   - plot_data æ•°é‡: {len(plot_data)}")
            for idx, item in enumerate(plot_data[:5], 1):  # åªæ˜¾ç¤ºå‰5ä¸ª
                logger.info(f"   - plot_data[{idx}]: {item}")
            
            human_prompt = f"""ç”¨æˆ·æŸ¥è¯¢ï¼š{user_query}

AIåˆ†æå¸ˆæ´å¯Ÿï¼š
{key_insight}

é»„é‡‘æ³•åˆ™ï¼š
{golden_rule}

è¯·åŸºäºä»¥ä¸Šæ´å¯Ÿï¼Œç”Ÿæˆä¸€ä¸ªæ¸…æ™°ã€æœ‰ç”¨çš„æ€»ç»“å›å¤ã€‚"""
            
            logger.info(f"ğŸ“Š [æ•°æ®è¯¦æƒ…] llm_summarize - å®Œæ•´ human_prompt å†…å®¹:\n{human_prompt}")
        else:
            # æ—§æµç¨‹ï¼šç›´æ¥æ±‡æ€»åŸå§‹æ•°æ®
            logger.info("ğŸ“Š [èŠ‚ç‚¹] llm_summarize - ä½¿ç”¨æ—§æµç¨‹ï¼ˆç›´æ¥æ±‡æ€»åŸå§‹æ•°æ®ï¼‰")
            
            system_prompt = """ä½ æ˜¯ä¸€ä¸ªæ•°æ®åˆ†æåŠ©æ‰‹ã€‚æ ¹æ®VikingDBæœç´¢ç»“æœå’ŒMySQLåˆ†æç»“æœï¼Œä¸ºç”¨æˆ·ç”Ÿæˆæ¸…æ™°ã€æœ‰ç”¨çš„æ€»ç»“ã€‚

é‡ç‚¹å…³æ³¨ï¼š
1. VikingDBæœç´¢åˆ°çš„å†…å®¹
2. MySQLæ•°æ®åˆ†æç»“æœï¼ˆå¦‚æœæœ‰ï¼‰
3. ç”¨æˆ·åŸå§‹æŸ¥è¯¢çš„ç­”æ¡ˆ

ç”¨è‡ªç„¶è¯­è¨€å›å¤ï¼Œä¸è¦è¿”å›JSONã€‚"""
            
            vkdb_info = json.dumps(vkdb_response, ensure_ascii=False, indent=2, cls=DateTimeJSONEncoder)
            mysql_info = ""
            mysql_join_result = state.get("mysql_join_result")
            if mysql_join_result:
                mysql_info = f"\n\nMySQLåˆ†æç»“æœï¼š\n{json.dumps(mysql_join_result, ensure_ascii=False, indent=2, cls=DateTimeJSONEncoder)}"
            
            user_query = state.get("vkdb_query") or "æŸ¥è¯¢"
            logger.info(f"ğŸ“ [èŠ‚ç‚¹è¾“å…¥] llm_summarize - ç”¨æˆ·æŸ¥è¯¢: {user_query}")
            
            human_prompt = f"""ç”¨æˆ·æŸ¥è¯¢ï¼š{user_query}

VikingDBæœç´¢ç»“æœï¼š
{vkdb_info}
{mysql_info}

è¯·ç”Ÿæˆæ€»ç»“å›å¤ã€‚"""
        
        # è¯Šæ–­ï¼šè¯¦ç»†åˆ†æprompté•¿åº¦ï¼Œå®šä½TTFTå»¶è¿Ÿé—®é¢˜
        system_prompt_len = len(system_prompt)
        human_prompt_len = len(human_prompt)
        prompt_length = system_prompt_len + human_prompt_len
        estimated_tokens = prompt_length // 4  # ç²—ç•¥ä¼°ç®—ï¼š1 token â‰ˆ 4 å­—ç¬¦
        
        logger.info(f"ğŸ“Š [èŠ‚ç‚¹è¯Šæ–­] llm_summarize - Promptç»„æˆåˆ†æ:")
        logger.info(f"   - System Prompt: {system_prompt_len:,} å­—ç¬¦")
        logger.info(f"   - Human Prompt: {human_prompt_len:,} å­—ç¬¦")
        logger.info(f"     * ç”¨æˆ·æŸ¥è¯¢éƒ¨åˆ†: {len(user_query)} å­—ç¬¦")
        if not has_analysis:
            vkdb_info_len = len(vkdb_info) if 'vkdb_info' in locals() else 0
            mysql_info_len = len(mysql_info) if 'mysql_info' in locals() else 0
            logger.info(f"     * VikingDBç»“æœ: {vkdb_info_len:,} å­—ç¬¦ ({vkdb_info_len/prompt_length*100:.1f}% of prompt)" if prompt_length > 0 else "     * VikingDBç»“æœ: 0 å­—ç¬¦")
            logger.info(f"     * MySQLç»“æœ: {mysql_info_len:,} å­—ç¬¦")
        else:
            logger.info(f"     * ä½¿ç”¨AIåˆ†ææ´å¯Ÿï¼ˆæ–°æµç¨‹ï¼‰")
        logger.info(f"   - Promptæ€»é•¿åº¦: {prompt_length:,} å­—ç¬¦")
        logger.info(f"   - ä¼°ç®—Tokenæ•°: ~{estimated_tokens:,} tokens")
        
        # å¦‚æœpromptå¾ˆå¤§ï¼Œç»™å‡ºè­¦å‘Š
        if estimated_tokens > 100000:
            logger.warning(f"âš ï¸ [èŠ‚ç‚¹è¯Šæ–­] llm_summarize - Promptéå¸¸å¤§ (~{estimated_tokens:,} tokens)ï¼Œå¯èƒ½å¯¼è‡´TTFTå»¶è¿Ÿ")
        elif estimated_tokens > 50000:
            logger.warning(f"âš ï¸ [èŠ‚ç‚¹è¯Šæ–­] llm_summarize - Promptè¾ƒå¤§ (~{estimated_tokens:,} tokens)ï¼Œå¯èƒ½å½±å“TTFT")
        
        try:
            import time
            prefill_start_time = time.time()
            logger.info("ğŸ”„ [èŠ‚ç‚¹æ‰§è¡Œ] llm_summarize - å¼€å§‹è°ƒç”¨LLM APIï¼ˆå³å°†è¿›å…¥Prefillé˜¶æ®µï¼‰...")
            logger.info(f"â±ï¸ [æ—¶é—´è¯Šæ–­] llm_summarize - APIè°ƒç”¨å¼€å§‹æ—¶é—´: {prefill_start_time:.3f}")
            summary_chunks: list[str] = []
            # å…³é”®ä¿®å¤ï¼šä¼ é€’ config ç»™ astreamï¼Œè®©æµå¼äº‹ä»¶èƒ½è¢« astream_events æ•è·
            async_iterator = llm.astream([
                SystemMessage(content=system_prompt),
                HumanMessage(content=human_prompt)
            ], config=config)
            
            # è®°å½•ç¬¬ä¸€ä¸ªchunkåˆ°è¾¾çš„æ—¶é—´
            first_chunk_time = None
            async for chunk in async_iterator:
                if first_chunk_time is None:
                    first_chunk_time = time.time()
                    ttft = first_chunk_time - prefill_start_time
                    logger.info(f"â±ï¸ [æ—¶é—´è¯Šæ–­] llm_summarize - ç¬¬ä¸€ä¸ªTokenåˆ°è¾¾æ—¶é—´: {first_chunk_time:.3f}")
                    logger.info(f"â±ï¸ [æ—¶é—´è¯Šæ–­] llm_summarize - TTFT (Time To First Token): {ttft:.2f} ç§’")
                if hasattr(chunk, "content") and chunk.content:
                    summary_chunks.append(chunk.content)
            summary = "".join(summary_chunks)
            logger.info(f"âœ… [èŠ‚ç‚¹ç»“æœ] llm_summarize - æ±‡æ€»å®Œæˆï¼Œé•¿åº¦: {len(summary)} å­—ç¬¦")
            
            # ç”Ÿæˆå›¾è¡¨å›¾ç‰‡å¹¶è¿½åŠ åˆ°æ–‡æœ¬ï¼ˆå¦‚æœæœ‰åˆ†æç»“æœï¼‰
            if has_analysis:
                plot_data = analysis_result.get("plot_data", [])
                if plot_data:
                    chart_markdown = _generate_chart_image_markdown(plot_data)
                    if chart_markdown:
                        summary += chart_markdown
                        logger.info(f"ğŸ“Š [èŠ‚ç‚¹ç»“æœ] llm_summarize - å›¾è¡¨å·²è¿½åŠ åˆ°æ–‡æœ¬ï¼ŒåŒ…å« {len(plot_data)} ä¸ªç±»åˆ«")
            
            return {
                "final_summary": summary
            }
        except Exception as e:
            logger.error(f"âŒ [èŠ‚ç‚¹é”™è¯¯] llm_summarize - æ±‡æ€»å¤±è´¥: {e}")
            return {
                "error": f"LLM summarization failed: {e}",
                "final_summary": None
            }
    
    return _node


def simple_chat_node(settings: AppSettings):
    """ç®€å•èŠå¤©èŠ‚ç‚¹ï¼šç›´æ¥LLMå›å¤"""
    
    llm = ChatTongyi(model=settings.qwen_model, temperature=settings.qwen_temperature)
    
    def _node(state: Dict[str, Any], config: RunnableConfig) -> Dict[str, Any]:
        import logging
        logger = logging.getLogger(__name__)
        logger.info("ğŸ” [èŠ‚ç‚¹æ‰§è¡Œ] simple_chat - å¼€å§‹ç®€å•èŠå¤©")
        
        # ç›´æ¥è®¿é—®å­—å…¸
        messages = state.get("messages", [])
        
        if not messages:
            logger.error("âŒ [èŠ‚ç‚¹é”™è¯¯] simple_chat - æ²¡æœ‰æ¶ˆæ¯")
            return {"error": "No messages provided"}
        
        user_input = messages[-1].content if messages else ""
        logger.info(f"ğŸ“ [èŠ‚ç‚¹è¾“å…¥] simple_chat - ç”¨æˆ·è¾“å…¥: {user_input[:50]}...")
        
        try:
            logger.info("ğŸ”„ [èŠ‚ç‚¹æ‰§è¡Œ] simple_chat - è°ƒç”¨LLMç”Ÿæˆå›å¤...")
            # ä¼ é€’ config ç»™ invokeï¼Œç¡®ä¿äº‹ä»¶èƒ½è¢«æ•è·ï¼ˆè™½ç„¶å½“å‰æ˜¯ invokeï¼Œä½†ä¸ºç»Ÿä¸€æ€§åŠ ä¸Šï¼‰
            response = llm.invoke(messages, config=config)
            content = response.content if hasattr(response, 'content') else str(response)
            
            logger.info(f"âœ… [èŠ‚ç‚¹ç»“æœ] simple_chat - å›å¤ç”Ÿæˆå®Œæˆï¼Œé•¿åº¦: {len(content)} å­—ç¬¦")
            
            return {
                "final_summary": content
            }
        except Exception as e:
            logger.error(f"âŒ [èŠ‚ç‚¹é”™è¯¯] simple_chat - èŠå¤©å¤±è´¥: {e}")
            return {
                "error": f"Simple chat failed: {e}",
                "final_summary": None
            }
    
    return _node


def intent_structurize_node(settings: AppSettings):
    """æ„å›¾ç»“æ„åŒ–èŠ‚ç‚¹ï¼šå°†æ®µè½å‹ intent_analysis è½¬ä¸ºç»“æ„åŒ– JSON"""
    
    def _node(state: Dict[str, Any]) -> Dict[str, Any]:
        import logging
        logger = logging.getLogger(__name__)
        logger.info("ğŸ” [èŠ‚ç‚¹æ‰§è¡Œ] intent_structurize - å¼€å§‹æ„å›¾ç»“æ„åŒ–")
        
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨
        if not settings.intent_structurize_enabled:
            logger.info("â„¹ï¸ [èŠ‚ç‚¹] intent_structurize - åŠŸèƒ½å·²ç¦ç”¨ï¼Œè·³è¿‡")
            return {}
        
        # ç›´æ¥è®¿é—®å­—å…¸
        vkdb_response = state.get("vkdb_response")
        if not vkdb_response:
            logger.error("âŒ [èŠ‚ç‚¹é”™è¯¯] intent_structurize - æ²¡æœ‰VikingDBæœç´¢ç»“æœ")
            return {"error": "No vkdb_response for intent structurization"}
        
        if state.get("vkdb_no_result"):
            logger.warning("âš ï¸ [èŠ‚ç‚¹] intent_structurize - ä¸Šæ¸¸æ— ç»“æœï¼Œè·³è¿‡")
            return {}
        
        try:
            logger.info("ğŸ”„ [èŠ‚ç‚¹æ‰§è¡Œ] intent_structurize - å¼€å§‹æ‰¹é‡ç»“æ„åŒ–å¤„ç†...")
            results = structurize_intents_batch(
                vkdb_response=vkdb_response,
                settings=settings,
                concurrency=settings.intent_structurize_concurrency,
                timeout=settings.intent_structurize_timeout
            )
            
            # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨ï¼ˆä¾¿äºå­˜å‚¨åˆ° stateï¼‰
            structured_list = []
            for result in results:
                structured_list.append({
                    "materialId": result.materialId,
                    "structured_intent": result.structured_intent,
                    "success": result.success,
                    "error": result.error
                })
            
            success_count = sum(1 for r in results if r.success)
            logger.info(f"âœ… [èŠ‚ç‚¹ç»“æœ] intent_structurize - å®Œæˆï¼ŒæˆåŠŸ: {success_count}/{len(results)}")
            
            return {
                "structured_intents": structured_list
            }
        except Exception as e:
            logger.error(f"âŒ [èŠ‚ç‚¹é”™è¯¯] intent_structurize - ç»“æ„åŒ–å¤±è´¥: {e}")
            return {
                "error": f"Intent structurization failed: {e}",
                "structured_intents": []
            }
    
    return _node


def data_aggregate_node(settings: AppSettings):
    """æ•°æ®èšåˆèŠ‚ç‚¹ï¼šåˆå¹¶ç»“æ„åŒ–æ„å›¾æ•°æ®ä¸ MySQL æ•°æ®ï¼Œç”Ÿæˆç»Ÿè®¡è¡¨"""
    
    def _node(state: Dict[str, Any]) -> Dict[str, Any]:
        import logging
        logger = logging.getLogger(__name__)
        logger.info("ğŸ” [èŠ‚ç‚¹æ‰§è¡Œ] data_aggregate - å¼€å§‹æ•°æ®èšåˆ")
        
        # ç›´æ¥è®¿é—®å­—å…¸
        structured_intents = state.get("structured_intents")
        mysql_join_result = state.get("mysql_join_result")
        
        if not structured_intents:
            logger.warning("âš ï¸ [èŠ‚ç‚¹] data_aggregate - æ²¡æœ‰ç»“æ„åŒ–æ„å›¾æ•°æ®ï¼Œè·³è¿‡")
            return {}
        
        if not mysql_join_result:
            logger.warning("âš ï¸ [èŠ‚ç‚¹] data_aggregate - æ²¡æœ‰MySQL Joinç»“æœï¼Œè·³è¿‡")
            return {}
        
        try:
            logger.info("ğŸ”„ [èŠ‚ç‚¹æ‰§è¡Œ] data_aggregate - å¼€å§‹èšåˆç»Ÿè®¡...")
            
            # è½¬æ¢ structured_intents ä¸º StructuredIntentResult å¯¹è±¡
            from src.services.intent_structurize_service import StructuredIntentResult
            intent_results = [
                StructuredIntentResult(
                    materialId=item["materialId"],
                    structured_intent=item["structured_intent"],
                    success=item.get("success", True),
                    error=item.get("error")
                )
                for item in structured_intents
            ]
            
            # ç”Ÿæˆ CSV
            csv_str = generate_aggregation_csv(
                structured_intents=intent_results,
                mysql_join_result=mysql_join_result,
                dimensions=["opening_strategy", "script_archetype", "closing_trigger"],
                min_count=settings.data_aggregate_min_count
            )
            
            logger.info(f"âœ… [èŠ‚ç‚¹ç»“æœ] data_aggregate - ç”Ÿæˆ CSVï¼Œé•¿åº¦: {len(csv_str)} å­—ç¬¦")
            logger.info(f"ğŸ“Š [æ•°æ®è¯¦æƒ…] data_aggregate - CSV å†…å®¹é¢„è§ˆï¼ˆå‰1000å­—ç¬¦ï¼‰:\n{csv_str[:1000]}")
            if len(csv_str) > 1000:
                logger.info(f"ğŸ“Š [æ•°æ®è¯¦æƒ…] data_aggregate - CSV å®Œæ•´å†…å®¹:\n{csv_str}")
            
            return {
                "aggregated_stats": csv_str
            }
        except Exception as e:
            logger.error(f"âŒ [èŠ‚ç‚¹é”™è¯¯] data_aggregate - èšåˆå¤±è´¥: {e}")
            return {
                "error": f"Data aggregation failed: {e}",
                "aggregated_stats": None
            }
    
    return _node


def llm_analyze_node(settings: AppSettings):
    """LLM åˆ†æèŠ‚ç‚¹ï¼šå¯¹èšåˆç»Ÿè®¡è¡¨è¿›è¡Œè¯­ä¹‰å½’çº³å’Œæ´å¯Ÿç”Ÿæˆ"""
    
    llm = ChatTongyi(model=settings.qwen_model, temperature=settings.qwen_temperature)
    
    def _node(state: Dict[str, Any]) -> Dict[str, Any]:
        import logging
        from pathlib import Path
        logger = logging.getLogger(__name__)
        logger.info("ğŸ” [èŠ‚ç‚¹æ‰§è¡Œ] llm_analyze - å¼€å§‹LLMåˆ†æ")
        
        # ç›´æ¥è®¿é—®å­—å…¸
        aggregated_stats = state.get("aggregated_stats")
        if not aggregated_stats:
            logger.warning("âš ï¸ [èŠ‚ç‚¹] llm_analyze - æ²¡æœ‰èšåˆç»Ÿè®¡æ•°æ®ï¼Œè·³è¿‡")
            return {}
        
        try:
            # åŠ è½½ summary_prompt æ¨¡æ¿
            prompt_path = Path(__file__).parent / "prompts" / "summary_prompt.md"
            prompt_template = prompt_path.read_text(encoding="utf-8")
            
            # æ›¿æ¢ CSV å ä½ç¬¦
            full_prompt = prompt_template.replace("{csv_context}", aggregated_stats)
            
            # ä½¿ç”¨ç»“æ„åŒ–è¾“å‡º
            class PlotDataItem(BaseModel):
                category: str
                count: int
                roi: float
                ctr: float
            
            class SummaryItem(BaseModel):
                key_insight: str = Field(description="æ·±åº¦åˆ†ææ–‡æœ¬ï¼Œå¯ä»¥ä½¿ç”¨Markdownæ ¼å¼")
                golden_rule: str = Field(description="ä¸€å¥è¯æ€»ç»“é«˜ROIå…¬å¼")
            
            class AnalysisOutput(BaseModel):
                summary: SummaryItem
                plot_data: TypingList[PlotDataItem]
            
            structured_llm = llm.with_structured_output(AnalysisOutput)
            
            logger.info("ğŸ”„ [èŠ‚ç‚¹æ‰§è¡Œ] llm_analyze - è°ƒç”¨LLMè¿›è¡Œè¯­ä¹‰å½’çº³...")
            try:
                response = structured_llm.invoke([
                    SystemMessage(content=full_prompt),
                    HumanMessage(content=aggregated_stats)
                ])
            except Exception as parse_error:
                # å¦‚æœç»“æ„åŒ–è¾“å‡ºå¤±è´¥ï¼Œè®°å½•è¯¦ç»†é”™è¯¯å¹¶å°è¯•fallback
                error_msg = str(parse_error)
                logger.warning(f"âš ï¸ [èŠ‚ç‚¹] llm_analyze - ç»“æ„åŒ–è¾“å‡ºå¤±è´¥: {error_msg[:200]}...")
                
                # å°è¯•ä½¿ç”¨æ™®é€šLLMè°ƒç”¨ï¼Œç„¶åæ‰‹åŠ¨è§£æJSON
                logger.info("ğŸ”„ [èŠ‚ç‚¹] llm_analyze - å°è¯•fallbackæ–¹æ¡ˆï¼ˆæ™®é€šLLMè°ƒç”¨+æ‰‹åŠ¨è§£æï¼‰")
                fallback_llm = ChatTongyi(model=settings.qwen_model, temperature=settings.qwen_temperature)
                
                # æ”¹è¿›promptï¼Œæ˜ç¡®è¦æ±‚JSONæ ¼å¼
                fallback_prompt = full_prompt + """

é‡è¦æç¤ºï¼š
- å¿…é¡»è¿”å›æœ‰æ•ˆçš„JSONæ ¼å¼
- key_insightå­—æ®µä¸­çš„æ¢è¡Œç¬¦å¿…é¡»ä½¿ç”¨\\nè½¬ä¹‰ï¼ˆä¸æ˜¯å®é™…çš„æ¢è¡Œï¼‰
- ä¸è¦ä½¿ç”¨Markdownä»£ç å—åŒ…è£¹JSON
- ç›´æ¥è¿”å›JSONå¯¹è±¡ï¼Œä¸è¦å…¶ä»–æ–‡å­—è¯´æ˜
"""
                
                fallback_response = fallback_llm.invoke([
                    SystemMessage(content=fallback_prompt),
                    HumanMessage(content=aggregated_stats)
                ])
                
                # å°è¯•ä»å“åº”ä¸­æå–JSONï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰
                content = fallback_response.content.strip()
                
                # ç§»é™¤å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
                if content.startswith("```json"):
                    content = content[7:]
                if content.startswith("```"):
                    content = content[3:]
                if content.endswith("```"):
                    content = content[:-3]
                content = content.strip()
                
                # å°è¯•æå–JSONå¯¹è±¡
                json_match = re.search(r'\{.*\}', content, re.DOTALL)
                if json_match:
                    try:
                        json_str = json_match.group()
                        # æ›¿æ¢æœªè½¬ä¹‰çš„æ¢è¡Œç¬¦
                        json_str = json_str.replace('\n', '\\n').replace('\r', '\\r')
                        parsed_json = json.loads(json_str)
                        
                        # éªŒè¯ç»“æ„
                        if 'summary' not in parsed_json or 'plot_data' not in parsed_json:
                            raise ValueError("JSONç»“æ„ä¸å®Œæ•´")
                        
                        # æ„é€ å“åº”å¯¹è±¡
                        class MockResponse:
                            def __init__(self, data):
                                summary_data = data.get('summary', {})
                                self.summary = type('obj', (object,), {
                                    'key_insight': summary_data.get('key_insight', ''),
                                    'golden_rule': summary_data.get('golden_rule', '')
                                })()
                                self.plot_data = [
                                    type('obj', (object,), {
                                        'category': item.get('category', ''),
                                        'count': item.get('count', 0),
                                        'roi': item.get('roi', 0.0),
                                        'ctr': item.get('ctr', 0.0)
                                    })()
                                    for item in data.get('plot_data', [])
                                ]
                        
                        response = MockResponse(parsed_json)
                        logger.info("âœ… [èŠ‚ç‚¹] llm_analyze - Fallbackæ–¹æ¡ˆæˆåŠŸè§£æJSON")
                    except (json.JSONDecodeError, ValueError) as e:
                        logger.error(f"âŒ [èŠ‚ç‚¹] llm_analyze - Fallback JSONè§£æå¤±è´¥: {e}")
                        logger.error(f"ğŸ“ [èŠ‚ç‚¹] llm_analyze - å“åº”å†…å®¹é¢„è§ˆ: {content[:500]}")
                        raise parse_error
                else:
                    logger.error(f"âŒ [èŠ‚ç‚¹] llm_analyze - æ— æ³•ä»å“åº”ä¸­æå–JSON")
                    logger.error(f"ğŸ“ [èŠ‚ç‚¹] llm_analyze - å“åº”å†…å®¹: {content[:500]}")
                    raise parse_error
            
            # è½¬æ¢ä¸ºå­—å…¸
            analysis_result = {
                "summary": {
                    "key_insight": response.summary.key_insight,
                    "golden_rule": response.summary.golden_rule
                },
                "plot_data": [
                    {
                        "category": item.category,
                        "count": item.count,
                        "roi": item.roi,
                        "ctr": item.ctr
                    }
                    for item in response.plot_data
                ]
            }
            
            logger.info(f"âœ… [èŠ‚ç‚¹ç»“æœ] llm_analyze - åˆ†æå®Œæˆï¼Œç”Ÿæˆ {len(response.plot_data)} ä¸ªç±»åˆ«")
            logger.info(f"ğŸ“Š [æ•°æ®è¯¦æƒ…] llm_analyze - key_insight å†…å®¹:\n{response.summary.key_insight}")
            logger.info(f"ğŸ“Š [æ•°æ®è¯¦æƒ…] llm_analyze - golden_rule: {response.summary.golden_rule}")
            logger.info(f"ğŸ“Š [æ•°æ®è¯¦æƒ…] llm_analyze - plot_data ({len(response.plot_data)} é¡¹):")
            for idx, item in enumerate(response.plot_data, 1):
                logger.info(f"   [{idx}] category={item.category}, count={item.count}, roi={item.roi:.3f}, ctr={item.ctr:.3f}")
            
            return {
                "analysis_result": analysis_result
            }
        except Exception as e:
            logger.error(f"âŒ [èŠ‚ç‚¹é”™è¯¯] llm_analyze - LLMåˆ†æå¤±è´¥: {e}")
            return {
                "error": f"LLM analysis failed: {e}",
                "analysis_result": None
            }
    
    return _node
