from __future__ import annotations

import json
from typing import AsyncIterator

from langchain_core.messages import HumanMessage, SystemMessage

from src.core.settings import load_settings
from src.graphs.agent_graph.graph import build_agent_graph
from src.graphs.agent_graph.state import AgentState


async def agent_stream(
    user_message: str,
    system_prompt: str | None = None
) -> AsyncIterator[str]:
    """
    Agentæµå¼æœåŠ¡
    
    ä½¿ç”¨Agent Graphå¤„ç†ç”¨æˆ·è¯·æ±‚ï¼Œé€šè¿‡astream_eventsæ•è·æµå¼äº‹ä»¶
    
    Args:
        user_message: ç”¨æˆ·è¾“å…¥çš„è‡ªç„¶è¯­è¨€æ¶ˆæ¯
        system_prompt: å¯é€‰çš„ç³»ç»Ÿæç¤ºè¯
    
    Yields:
        SSEæ ¼å¼çš„å­—ç¬¦ä¸²ï¼ˆdata: {...}\n\nï¼‰
    """
    import logging
    logger = logging.getLogger(__name__)
    
    logger.info(f"ğŸš€ [æœåŠ¡å¯åŠ¨] agent_stream - æ”¶åˆ°è¯·æ±‚: {user_message[:50]}...")
    
    settings = load_settings()
    graph = build_agent_graph(settings)
    
    # æ„å»ºåˆå§‹æ¶ˆæ¯
    messages = []
    if system_prompt:
        messages.append(SystemMessage(content=system_prompt))
    messages.append(HumanMessage(content=user_message))
    
    # åˆå§‹çŠ¶æ€ï¼ˆç›´æ¥ä½¿ç”¨å­—å…¸æ ¼å¼ï¼‰
    initial_state = {
        "messages": messages,
        "intent": None,
        "vkdb_query": None,
        "vkdb_influencer": None,
        "vkdb_response": None,
        "vkdb_no_result": None,
        "mysql_join_result": None,
        "final_summary": None,
        "error": None,
    }
    
    try:
        # å‘é€çŠ¶æ€æ¶ˆæ¯
        status_payload = json.dumps(
            {"type": "status", "content": "æ­£åœ¨åˆ†ææ‚¨çš„è¯·æ±‚..."},
            ensure_ascii=False
        )
        yield f"data: {status_payload}\n\n"
        
        # ä½¿ç”¨astream_eventsè·å–æµå¼äº‹ä»¶
        logger.info("ğŸ”„ [æœåŠ¡æ‰§è¡Œ] agent_stream - å¼€å§‹æ‰§è¡ŒGraph...")
        final_state = None
        streamed_token = False  # æ ‡è®°æ˜¯å¦å·²æœ‰æµå¼tokenè¾“å‡ºï¼Œé¿å…é‡å¤
        token_count = 0  # æµå¼tokenè®¡æ•°å™¨
        stream_started = False  # æ ‡è®°æµå¼æ˜¯å¦å·²å¼€å§‹
        stream_end_logged = False  # æ ‡è®°æ˜¯å¦å·²è¾“å‡ºç»“å°¾æ—¥å¿—
        total_tokens_estimate = None  # ä¼°ç®—æ€»tokenæ•°ï¼ˆç”¨äºåˆ¤æ–­ä¸­é—´ä½ç½®ï¼‰
        accumulated_text = ""  # è®°å½•å·²å‘é€çš„æ–‡æœ¬å†…å®¹ï¼Œç”¨äºæ£€æµ‹å›¾è¡¨å¢é‡
        
        async for event in graph.astream_events(initial_state, version="v1"):
            event_type = event.get("event", "")
            event_name = event.get("name", "")
            
            # è®°å½•æ‰€æœ‰é‡è¦äº‹ä»¶ï¼ˆæ”¹ä¸ºINFOçº§åˆ«ä»¥ä¾¿è°ƒè¯•ï¼‰
            if event_type in ["on_chain_start", "on_tool_start", "on_chain_end"]:
                metadata = event.get("metadata", {})
                node_name = metadata.get("langgraph_node", "")
                logger.info(f"ğŸ“¡ [äº‹ä»¶] {event_type} - {event_name}, node: {node_name}")
            
            # å¤„ç†LLMæµå¼è¾“å‡º
            if event_type == "on_chat_model_stream":
                # å…³é”®ä¿®å¤ï¼šè¿‡æ»¤èŠ‚ç‚¹åç§°ï¼Œåªå¤„ç† llm_summarize å’Œ simple_chat çš„æµå¼è¾“å‡º
                metadata = event.get("metadata", {})
                node_name = metadata.get("langgraph_node", "")
                
                # åªå¤„ç†æ±‡æ€»èŠ‚ç‚¹å’Œç®€å•èŠå¤©èŠ‚ç‚¹çš„æµå¼è¾“å‡ºï¼Œé¿å…æ··å…¥å…¶ä»–èŠ‚ç‚¹çš„æ€è€ƒè¿‡ç¨‹
                if node_name in ["llm_summarize", "simple_chat"]:
                    chunk = event.get("data", {}).get("chunk")
                    if chunk and hasattr(chunk, "content") and chunk.content:
                        streamed_token = True
                        accumulated_text += chunk.content  # ç´¯ç§¯å·²å‘é€çš„æ–‡æœ¬
                        payload = json.dumps(
                            {"type": "token", "content": chunk.content},
                            ensure_ascii=False
                        )
                        yield f"data: {payload}\n\n"
                        
                        token_count += 1
                        
                        # æµå¼å¼€å§‹ï¼šè¾“å‡ºç¬¬1æ¡æ—¥å¿—
                        if not stream_started:
                            stream_started = True
                            logger.info(f"ğŸ“¤ [æœåŠ¡] agent_stream - æµå¼å¼€å§‹: {chunk.content[:30]}... (from {node_name})")
                        
                        # ä¸­é—´ä½ç½®ï¼šåœ¨token_countè¾¾åˆ°ä¸€å®šæ•°é‡æ—¶è¾“å‡º2æ¡ï¼ˆæ¯”å¦‚æ¯100ä¸ªtokenè¾“å‡ºä¸€æ¬¡ï¼Œæ€»å…±è¾“å‡º2æ¬¡ï¼‰
                        elif token_count == 10:  # ç¬¬10ä¸ªtokenæ—¶è¾“å‡ºç¬¬1æ¡ä¸­é—´æ—¥å¿—
                            logger.info(f"ğŸ“¤ [æœåŠ¡] agent_stream - æµå¼ä¼ è¾“ä¸­ [{token_count}]: {chunk.content[:30]}...")
                        elif token_count == 20:  # ç¬¬20ä¸ªtokenæ—¶è¾“å‡ºç¬¬2æ¡ä¸­é—´æ—¥å¿—
                            logger.info(f"ğŸ“¤ [æœåŠ¡] agent_stream - æµå¼ä¼ è¾“ä¸­ [{token_count}]: {chunk.content[:30]}...")
            
            # æ•è·èŠ‚ç‚¹ç»“æŸäº‹ä»¶ï¼Œè·å–æœ€ç»ˆçŠ¶æ€
            elif event_type == "on_chain_end":
                metadata = event.get("metadata", {})
                node_name = metadata.get("langgraph_node", "")
                
                # æ–¹æ³•1ï¼šæ£€æŸ¥æ˜¯å¦æ˜¯æœ€åä¸€ä¸ªèŠ‚ç‚¹çš„ç»“æŸï¼ˆllm_summarize æˆ– simple_chatï¼‰
                if node_name in ["llm_summarize", "simple_chat"]:
                    # è·å–èŠ‚ç‚¹è¾“å‡º
                    output = event.get("data", {}).get("output", {})
                    logger.info(f"ğŸ” [è°ƒè¯•] agent_stream - æ•è·åˆ° {node_name} èŠ‚ç‚¹ç»“æŸï¼Œoutput ç±»å‹: {type(output)}, æ˜¯å¦ä¸ºdict: {isinstance(output, dict)}")
                    if isinstance(output, dict):
                        final_state = output
                        logger.info(f"ğŸ” [è°ƒè¯•] agent_stream - è®¾ç½® final_stateï¼ŒåŒ…å«å­—æ®µ: {list(final_state.keys())}")
                        
                        # ç»“å°¾æ—¥å¿—ï¼šè¾“å‡º3æ¡ï¼ˆåœ¨èŠ‚ç‚¹ç»“æŸæ—¶ï¼‰
                        if stream_started and not stream_end_logged:
                            stream_end_logged = True
                            final_summary = output.get("final_summary", "")
                            summary_len = len(final_summary) if final_summary else 0
                            logger.info(f"ğŸ“¤ [æœåŠ¡] agent_stream - æµå¼ç»“æŸ [æ€»è®¡ {token_count} tokens, å†…å®¹ {summary_len} å­—ç¬¦]")
                            logger.info(f"ğŸ“‹ [æœåŠ¡] agent_stream - æ•è·èŠ‚ç‚¹æœ€ç»ˆçŠ¶æ€ ({node_name})ï¼ŒåŒ…å«å­—æ®µ: {list(final_state.keys())}")
                            logger.info(f"âœ… [æœåŠ¡] agent_stream - æµå¼ä¼ è¾“å®Œæˆ")
                    else:
                        logger.warning(f"âš ï¸ [è°ƒè¯•] agent_stream - {node_name} èŠ‚ç‚¹è¾“å‡ºæ ¼å¼ä¸æ­£ç¡®ï¼Œoutput: {output}")
                
                # æ–¹æ³•2ï¼šæ£€æŸ¥æ˜¯å¦æ˜¯æ•´ä¸ª Graph çš„ç»“æŸï¼ˆRoot Runï¼Œæ²¡æœ‰ langgraph_nodeï¼‰
                elif not node_name:
                    # Graph çº§åˆ«çš„ç»“æŸäº‹ä»¶
                    output = event.get("data", {}).get("output", {})
                    if isinstance(output, dict) and "final_summary" in output:
                        final_state = output
                        # å¦‚æœä¹‹å‰æ²¡æœ‰è¾“å‡ºç»“å°¾æ—¥å¿—ï¼Œåœ¨è¿™é‡Œè¾“å‡º
                        if stream_started and not stream_end_logged:
                            stream_end_logged = True
                            final_summary = output.get("final_summary", "")
                            summary_len = len(final_summary) if final_summary else 0
                            logger.info(f"ğŸ“¤ [æœåŠ¡] agent_stream - æµå¼ç»“æŸ [æ€»è®¡ {token_count} tokens, å†…å®¹ {summary_len} å­—ç¬¦]")
                            logger.info(f"ğŸ“‹ [æœåŠ¡] agent_stream - æ•è·Graphæœ€ç»ˆçŠ¶æ€ï¼ŒåŒ…å«å­—æ®µ: {list(final_state.keys())}")
                            logger.info(f"âœ… [æœåŠ¡] agent_stream - æµå¼ä¼ è¾“å®Œæˆ")
                        else:
                            logger.info(f"ğŸ“‹ [æœåŠ¡] agent_stream - æ•è·Graphæœ€ç»ˆçŠ¶æ€ï¼ŒåŒ…å«å­—æ®µ: {list(final_state.keys())}")
            
            # å¤„ç†å·¥å…·è°ƒç”¨å¼€å§‹
            elif event_type == "on_tool_start":
                tool_name = event.get("name", "unknown")
                payload = json.dumps(
                    {"type": "status", "content": f"æ­£åœ¨è°ƒç”¨å·¥å…·: {tool_name}..."},
                    ensure_ascii=False
                )
                yield f"data: {payload}\n\n"
            
            # å¤„ç†èŠ‚ç‚¹å¼€å§‹
            elif event_type == "on_chain_start":
                node_name = event.get("name", "")
                if "intent_analysis" in node_name.lower():
                    payload = json.dumps(
                        {"type": "status", "content": "æ­£åœ¨åˆ†ææ„å›¾..."},
                        ensure_ascii=False
                    )
                    yield f"data: {payload}\n\n"
                elif "vkdb_search" in node_name.lower():
                    payload = json.dumps(
                        {"type": "status", "content": "æ­£åœ¨æœç´¢VikingDB..."},
                        ensure_ascii=False
                    )
                    yield f"data: {payload}\n\n"
                elif "mysql_join" in node_name.lower():
                    payload = json.dumps(
                        {"type": "status", "content": "æ­£åœ¨æ‰§è¡ŒMySQLåˆ†æ..."},
                        ensure_ascii=False
                    )
                    yield f"data: {payload}\n\n"
                elif "llm_summarize" in node_name.lower():
                    payload = json.dumps(
                        {"type": "status", "content": "æ­£åœ¨ç”Ÿæˆæ€»ç»“..."},
                        ensure_ascii=False
                    )
                    yield f"data: {payload}\n\n"
                elif "simple_chat" in node_name.lower():
                    payload = json.dumps(
                        {"type": "status", "content": "æ­£åœ¨æ€è€ƒ..."},
                        ensure_ascii=False
                    )
                    yield f"data: {payload}\n\n"
        
        # Graphæ‰§è¡Œå®Œæˆåï¼Œæ£€æŸ¥final_summaryå¹¶æµå¼è¾“å‡ºï¼ˆå…œåº•é€»è¾‘ï¼‰
        # æ£€æŸ¥æ˜¯å¦æœ‰å›¾è¡¨å¢é‡éœ€è¦è¡¥å‘
        logger.info(f"ğŸ” [è°ƒè¯•] agent_stream - å¾ªç¯ç»“æŸï¼Œfinal_state: {final_state is not None}, streamed_token: {streamed_token}, accumulated_texté•¿åº¦: {len(accumulated_text)}")
        if final_state and streamed_token:
            final_summary = final_state.get("final_summary", "")
            logger.info(f"ğŸ” [è°ƒè¯•] agent_stream - final_summaryå­˜åœ¨: {final_summary is not None}, é•¿åº¦: {len(final_summary) if final_summary else 0}")
            if final_summary and len(final_summary) > len(accumulated_text):
                # è®¡ç®—å·®å€¼ï¼ˆå³å›¾è¡¨ Markdown éƒ¨åˆ†ï¼‰
                chart_part = final_summary[len(accumulated_text):]
                if chart_part.strip():
                    logger.info(f"ğŸ“Š [æœåŠ¡] agent_stream - æ£€æµ‹åˆ°å›¾è¡¨å¢é‡ï¼Œæ­£åœ¨è¡¥å‘ (é•¿åº¦: {len(chart_part)} å­—ç¬¦)")
                    payload = json.dumps(
                        {"type": "token", "content": chart_part},
                        ensure_ascii=False
                    )
                    yield f"data: {payload}\n\n"
            elif final_summary:
                logger.info(f"ğŸ” [è°ƒè¯•] agent_stream - final_summaryé•¿åº¦ ({len(final_summary)}) ä¸å¤§äº accumulated_texté•¿åº¦ ({len(accumulated_text)})ï¼Œæ— éœ€è¡¥å‘")
        elif not final_state:
            logger.warning(f"âš ï¸ [è°ƒè¯•] agent_stream - final_state ä¸º Noneï¼Œæ— æ³•æ£€æŸ¥å›¾è¡¨å¢é‡")
        elif not streamed_token:
            logger.info(f"ğŸ” [è°ƒè¯•] agent_stream - æ²¡æœ‰æµå¼tokenï¼Œè·³è¿‡å›¾è¡¨å¢é‡æ£€æŸ¥")
        
        # Graphæ‰§è¡Œå®Œæˆåï¼Œæ£€æŸ¥final_summaryå¹¶æµå¼è¾“å‡ºï¼ˆå…œåº•é€»è¾‘ï¼‰
        # åªæœ‰åœ¨æœªäº§ç”Ÿè¿‡æµå¼tokenæ—¶ï¼Œæ‰ç”¨final_summaryå…œåº•è¾“å‡ºï¼Œé¿å…é‡å¤
        if not streamed_token:
            final_summary = None
            
            # æ–¹æ³•1ï¼šä»äº‹ä»¶ä¸­æ•è·çš„æœ€ç»ˆçŠ¶æ€è·å–
            if final_state:
                final_summary = final_state.get("final_summary")
                if final_summary:
                    logger.info(f"ğŸ“¤ [æœåŠ¡] agent_stream - ä»äº‹ä»¶çŠ¶æ€è¾“å‡ºfinal_summaryå…œåº•: {final_summary[:50]}...")
            
            # æ–¹æ³•2ï¼šå¦‚æœäº‹ä»¶ä¸­æœªæ‰¾åˆ°ï¼Œä»å®Œæ•´GraphçŠ¶æ€è·å–
            if not final_summary:
                logger.warning("âš ï¸ [æœåŠ¡] agent_stream - æœªä»äº‹ä»¶ä¸­è·å–åˆ°final_summaryï¼Œå°è¯•ä»GraphçŠ¶æ€è·å–")
                async for state in graph.astream(initial_state):
                    final_summary = state.get("final_summary")
                    if final_summary:
                        logger.info(f"ğŸ“¤ [æœåŠ¡] agent_stream - ä»GraphçŠ¶æ€è·å–final_summary: {final_summary[:50]}...")
                        break
            
            # è¾“å‡ºå…œåº•å†…å®¹
            if final_summary:
                for char in final_summary:
                    payload = json.dumps(
                        {"type": "token", "content": char},
                        ensure_ascii=False
                    )
                    yield f"data: {payload}\n\n"
            else:
                logger.warning("âš ï¸ [æœåŠ¡] agent_stream - æœªæ‰¾åˆ°final_summaryï¼Œæ— æ³•è¾“å‡ºå…œåº•å†…å®¹")
        
        # æµç»“æŸä¿¡å·
        yield "data: [DONE]\n\n"
    
    except Exception as e:
        import traceback
        error_msg = f"å¤„ç†æ¶ˆæ¯æ—¶å‡ºé”™: {str(e)}\n{traceback.format_exc()}"
        error_payload = json.dumps(
            {"type": "error", "content": error_msg},
            ensure_ascii=False
        )
        yield f"data: {error_payload}\n\n"
        yield "data: [DONE]\n\n"
