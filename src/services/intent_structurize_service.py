from __future__ import annotations

import json
import logging
import time
from concurrent.futures import ThreadPoolExecutor, as_completed, TimeoutError as FutureTimeoutError
from pathlib import Path
from typing import Any, Dict, List, Optional

from langchain_core.messages import HumanMessage, SystemMessage
from langchain_community.chat_models import ChatTongyi
from pydantic import BaseModel

from src.core.settings import AppSettings
from src.infra.vkdb.join import extract_join_info_from_vkdb_item

logger = logging.getLogger(__name__)


class StructuredIntentResult(BaseModel):
    """ç»“æ„åŒ–æ„å›¾è§£æç»“æœ"""
    materialId: str
    structured_intent: Dict[str, Any]
    success: bool
    error: Optional[str] = None


def load_intent_prompt() -> str:
    """åŠ è½½æ„å›¾ç»“æ„åŒ– Prompt æ¨¡æ¿"""
    prompt_path = Path(__file__).parent.parent / "graphs" / "agent_graph" / "prompts" / "intent_prompt.md"
    try:
        return prompt_path.read_text(encoding="utf-8")
    except Exception as e:
        logger.error(f"Failed to load intent_prompt.md: {e}")
        raise


def parse_single_intent_analysis(
    material_id: str,
    intent_analysis: str,
    llm: ChatTongyi,
    prompt_template: str,
    timeout: int = 20
) -> StructuredIntentResult:
    """
    è§£æå•ä¸ªè§†é¢‘çš„æ„å›¾åˆ†ææ–‡æœ¬
    
    Args:
        material_id: ç´ æIDï¼ˆç”¨äºæ—¥å¿—å’Œç»“æœç»‘å®šï¼‰
        intent_analysis: æ®µè½å‹æ„å›¾åˆ†ææ–‡æœ¬
        llm: LLMå®ä¾‹
        prompt_template: Promptæ¨¡æ¿
        timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    
    Returns:
        StructuredIntentResult
    """
    if not intent_analysis or not intent_analysis.strip():
        return StructuredIntentResult(
            materialId=material_id,
            structured_intent={},
            success=False,
            error="Empty intent_analysis"
        )
    
    try:
        # ä½¿ç”¨ç»“æ„åŒ–è¾“å‡º
        class IntentAnalysisOutput(BaseModel):
            narrative_analysis: Dict[str, Any]
            tactical_breakdown: Dict[str, Any]
            innovation_check: Dict[str, Any]
        
        structured_llm = llm.with_structured_output(IntentAnalysisOutput)
        
        # è°ƒç”¨ LLMï¼ˆå¸¦è¶…æ—¶æ§åˆ¶ï¼‰
        start_time = time.time()
        response = structured_llm.invoke([
            SystemMessage(content=prompt_template),
            HumanMessage(content=intent_analysis)
        ])
        elapsed = time.time() - start_time
        
        if elapsed > timeout:
            logger.warning(f"âš ï¸ [ç»“æ„åŒ–] materialId={material_id} - å¤„ç†æ—¶é—´ {elapsed:.2f}s è¶…è¿‡è¶…æ—¶é˜ˆå€¼ {timeout}s")
        
        # è½¬æ¢ä¸ºå­—å…¸
        structured_dict = {
            "narrative_analysis": response.narrative_analysis,
            "tactical_breakdown": response.tactical_breakdown,
            "innovation_check": response.innovation_check
        }
        
        logger.info(f"âœ… [ç»“æ„åŒ–] materialId={material_id} - è§£ææˆåŠŸï¼Œè€—æ—¶ {elapsed:.2f}s")
        return StructuredIntentResult(
            materialId=material_id,
            structured_intent=structured_dict,
            success=True
        )
    
    except Exception as e:
        logger.error(f"âŒ [ç»“æ„åŒ–] materialId={material_id} - è§£æå¤±è´¥: {e}")
        return StructuredIntentResult(
            materialId=material_id,
            structured_intent={},
            success=False,
            error=str(e)
        )


def extract_videos_from_vkdb_response(vkdb_response: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    ä» VikingDB å“åº”ä¸­æå–è§†é¢‘æ•°æ®
    
    Returns:
        List[{"materialId": str, "intent_analysis": str}]
    """
    result = vkdb_response.get("result", {})
    data = result.get("data", [])
    
    videos = []
    for item in data:
        if not isinstance(item, dict):
            continue
        
        fields = item.get("fields", {})
        intent_analysis = fields.get("intent_analysis", "")
        
        # æå– materialId
        join_info = extract_join_info_from_vkdb_item(item)
        material_id = join_info.material_id
        
        if not material_id:
            logger.warning(f"âš ï¸ [æå–] è·³è¿‡æ—  materialId çš„è®°å½•")
            continue
        
        if not intent_analysis or not intent_analysis.strip():
            logger.warning(f"âš ï¸ [æå–] materialId={material_id} - æ—  intent_analysisï¼Œè·³è¿‡")
            continue
        
        videos.append({
            "materialId": material_id,
            "intent_analysis": intent_analysis
        })
    
    return videos


def structurize_intents_batch(
    vkdb_response: Dict[str, Any],
    settings: AppSettings,
    concurrency: Optional[int] = None,
    timeout: Optional[int] = None
) -> List[StructuredIntentResult]:
    """
    æ‰¹é‡å¹¶å‘è§£ææ„å›¾åˆ†æ
    
    Args:
        vkdb_response: VikingDB æœç´¢ç»“æœ
        settings: åº”ç”¨é…ç½®
        concurrency: å¹¶å‘æ•°ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®å€¼ï¼‰
        timeout: å•æ¡è¶…æ—¶æ—¶é—´ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®å€¼ï¼‰
    
    Returns:
        List[StructuredIntentResult]
    """
    concurrency = concurrency or settings.intent_structurize_concurrency
    timeout = timeout or settings.intent_structurize_timeout
    
    # æå–è§†é¢‘æ•°æ®
    videos = extract_videos_from_vkdb_response(vkdb_response)
    total_count = len(videos)
    
    if total_count == 0:
        logger.warning("âš ï¸ [æ‰¹é‡ç»“æ„åŒ–] æ²¡æœ‰å¯è§£æçš„è§†é¢‘æ•°æ®")
        return []
    
    logger.info(f"ğŸš€ [æ‰¹é‡ç»“æ„åŒ–] å¼€å§‹å¤„ç† {total_count} æ¡æ•°æ®ï¼Œå¹¶å‘æ•°: {concurrency}, è¶…æ—¶: {timeout}s")
    
    # åŠ è½½ Prompt æ¨¡æ¿
    prompt_template = load_intent_prompt()
    
    # åˆå§‹åŒ– LLM
    llm = ChatTongyi(
        model=settings.qwen_model,
        temperature=settings.qwen_temperature
    )
    
    # å¹¶å‘å¤„ç†
    results: List[StructuredIntentResult] = []
    start_time = time.time()
    
    with ThreadPoolExecutor(max_workers=concurrency) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_video = {
            executor.submit(
                parse_single_intent_analysis,
                video["materialId"],
                video["intent_analysis"],
                llm,
                prompt_template,
                timeout
            ): video["materialId"]
            for video in videos
        }
        
        # æ”¶é›†ç»“æœ
        completed = 0
        for future in as_completed(future_to_video):
            material_id = future_to_video[future]
            try:
                result = future.result(timeout=timeout + 5)  # é¢å¤–5ç§’ç¼“å†²
                results.append(result)
                completed += 1
                
                if completed % 10 == 0:
                    logger.info(f"ğŸ“Š [æ‰¹é‡ç»“æ„åŒ–] è¿›åº¦: {completed}/{total_count}")
            
            except FutureTimeoutError:
                logger.error(f"âŒ [æ‰¹é‡ç»“æ„åŒ–] materialId={material_id} - ä»»åŠ¡è¶…æ—¶")
                results.append(StructuredIntentResult(
                    materialId=material_id,
                    structured_intent={},
                    success=False,
                    error="Task timeout"
                ))
            except Exception as e:
                logger.error(f"âŒ [æ‰¹é‡ç»“æ„åŒ–] materialId={material_id} - ä»»åŠ¡å¼‚å¸¸: {e}")
                results.append(StructuredIntentResult(
                    materialId=material_id,
                    structured_intent={},
                    success=False,
                    error=str(e)
                ))
    
    elapsed = time.time() - start_time
    success_count = sum(1 for r in results if r.success)
    success_rate = (success_count / total_count * 100) if total_count > 0 else 0
    
    logger.info(f"âœ… [æ‰¹é‡ç»“æ„åŒ–] å®Œæˆï¼æ€»è€—æ—¶: {elapsed:.2f}s, æˆåŠŸ: {success_count}/{total_count} ({success_rate:.1f}%)")
    
    # æ€§èƒ½æ£€æŸ¥
    if elapsed > 30:
        logger.warning(f"âš ï¸ [æ‰¹é‡ç»“æ„åŒ–] æ€»è€—æ—¶ {elapsed:.2f}s è¶…è¿‡30ç§’ç›®æ ‡")
    
    return results

